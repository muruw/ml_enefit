{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import enefit\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-12-14T09:54:39.569006Z","iopub.execute_input":"2025-12-14T09:54:39.569486Z","iopub.status.idle":"2025-12-14T09:54:40.009747Z","shell.execute_reply.started":"2025-12-14T09:54:39.569422Z","shell.execute_reply":"2025-12-14T09:54:40.008564Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport xgboost as xgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:54:40.011588Z","iopub.execute_input":"2025-12-14T09:54:40.012033Z","iopub.status.idle":"2025-12-14T09:54:40.819018Z","shell.execute_reply.started":"2025-12-14T09:54:40.012004Z","shell.execute_reply":"2025-12-14T09:54:40.817566Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\ndf_data = pd.read_csv(DATA_DIR + \"train.csv\")\ndf_client = pd.read_csv(DATA_DIR + \"client.csv\")\ndf_historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\ndf_forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\ndf_electricity_prices = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\ndf_gas_prices = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\ndf_weather_station_to_county_mapping = pd.read_csv(DATA_DIR + \"weather_station_to_county_mapping.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:54:40.820609Z","iopub.execute_input":"2025-12-14T09:54:40.820949Z","iopub.status.idle":"2025-12-14T09:55:02.596832Z","shell.execute_reply.started":"2025-12-14T09:54:40.820921Z","shell.execute_reply":"2025-12-14T09:55:02.595173Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def dbg(name, df):\n    print(f\"{name}: {len(df):,} rows\")\n\ndef upsample_daily_to_hourly(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n    # unsample (process) client data. Client data is reported daily, but we need it hourly.\n    # So we take ever row (1 day) and duplicate (explode) it into 24 rows\n    df_hourly = df.copy(deep=True) # deep copy to not make changes to dataframe from parameter\n    df_hourly[date_col] = pd.to_datetime(df_hourly[date_col])\n\n    # create a column of lists, where each list contains [00:00, 01:00, ... 23:00] for that day\n    df_hourly['datetime'] = df_hourly[date_col].apply(lambda x: [x + pd.Timedelta(hours=i) for i in range(24)])\n    df_hourly = df_hourly.explode('datetime')\n\n    # drop the original daily date column as it's no longer needed\n    df_hourly = df_hourly.drop(columns=[date_col])\n    return df_hourly\n\n\n# nb! needs more testing as this method was generated by AI.\ndef process_forecast_weather(df_forecast: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n    \"\"\"\n    Cleans, aggregates, and pivots forecast weather data.\n\n    Logic:\n    1. Map Lat/Lon to County ID.\n    2. Convert 'Origin Time' (when forecast was made) to 'Target Time' (when weather happens).\n    3. Group forecasts into 'batches' (Day 1 forecast vs Day 2 forecast).\n    4. Average the values per county/hour/batch.\n    5. Pivot so batches become columns (e.g., temperature_1, temperature_2).\n    \"\"\"\n    # Safety Copy\n    df = df_forecast.copy(deep=True)\n\n    # 1. Map Coordinates to County\n    # We use the dictionary passed in from the main function\n    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n\n    # Filter out valid locations only\n    df = df[df['county'] != -1]\n\n    # 2. Calculate Target Time\n    # Standardize origin time to 02:00:00 (removes minute/second noise)\n    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'])\n    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'].dt.date.astype(str) + ' 02:00:00')\n\n    # Target Time = Origin + Hours Ahead\n    df['forecast_datetime'] = df['origin_datetime'] + pd.to_timedelta(df['hours_ahead'], unit='h')\n\n    # We don't need origin time anymore\n    df.drop(columns=['origin_datetime'], inplace=True)\n\n    # 3. Create \"Batches\" (cumcount) todo:rename\n    # A batch represents how far out the forecast is (Day 1 vs Day 2)\n    # (hours_ahead - 1) // 24 + 1 results in: 1 for 0-24h, 2 for 25-48h\n    df['cumcount'] = (df['hours_ahead'] - 1) // 24 + 1\n\n    # 4. Aggregate (Mean) by County, Time, and Batch\n    # Identify feature columns (exclude IDs and Time)\n    exclude_cols = ['latitude', 'longitude', 'hours_ahead', 'forecast_datetime', 'cumcount', 'county', 'data_block_id']\n    feature_cols = [col for col in df.columns if col not in exclude_cols]\n\n    agg_dict = {col: 'mean' for col in feature_cols}\n    # We must keep 'cumcount' in the groupby keys, so we don't aggregate it\n\n    df_grouped = df.groupby(['county', 'forecast_datetime', 'cumcount']).agg(agg_dict)\n\n    # 5. Pivot (Unstack)\n    # Moves 'cumcount' from a row index to a column suffix\n    df_pivoted = df_grouped.unstack(level=-1)\n\n    # Flatten MultiIndex columns: ('temperature', 1) -> 'temperature_1'\n    df_pivoted.columns = [f'{col[0]}_{col[1]}' for col in df_pivoted.columns]\n\n    df_pivoted.reset_index(inplace=True)\n    df_pivoted.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n\n    # Handle missing values (if a forecast is missing, fill with 0 or strictly manage it)\n    df_pivoted.fillna(0, inplace=True)\n\n    return df_pivoted\n\ndef process_historical_weather(df_historical: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n    #compose historical weather data (averaging stations per county).\n    df = df_historical.copy(deep=True)\n    df['datetime'] = pd.to_datetime(df['datetime'])\n\n    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n    df = df[df['county'] != -1]\n\n    exclude_cols = ['latitude', 'longitude', 'datetime', 'county', 'data_block_id']\n    agg_dict = {col: 'mean' for col in df.columns if col not in exclude_cols}\n    df_grouped = df.groupby(['county', 'datetime']).agg(agg_dict)\n    df_grouped.reset_index(inplace=True)\n\n    return df_grouped\n\n\n  #### CHANGE HERE NEW FUNCTION ####\ndef add_time_features(df: pd.DataFrame, datetime_col: str = \"datetime\") -> pd.DataFrame:\n    out = df.copy(deep=True)\n    dt = pd.to_datetime(out[datetime_col])\n\n    # Basic calendar parts\n    out[\"hour\"] = dt.dt.hour.astype(\"int8\")\n    out[\"dayofweek\"] = dt.dt.dayofweek.astype(\"int8\")   \n    out[\"month\"] = dt.dt.month.astype(\"int8\")          \n    out[\"day\"] = dt.dt.day.astype(\"int8\")          \n    out[\"is_weekend\"] = (out[\"dayofweek\"] >= 5).astype(\"int8\")\n\n    # Cyclical encodings\n    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n\n    out[\"month_sin\"] = np.sin(2 * np.pi * out[\"month\"] / 12)\n    out[\"month_cos\"] = np.cos(2 * np.pi * out[\"month\"] / 12)\n\n    return out\n\ndef generate_features(\n    df_data: pd.DataFrame,\n    df_client: pd.DataFrame,\n    df_gas_prices: pd.DataFrame,\n    df_electricity_prices: pd.DataFrame,\n    df_historical_weather: pd.DataFrame,\n    df_forecast_weather: pd.DataFrame,\n    df_weather_station_to_county_mapping: pd.DataFrame,\n    train_start = '2021-09-01 11:00:00'):\n  \n  print(dbg(\"start\", df_data))\n\n  # The weather data uses Latitude/Longitude, but the energy data uses \"Counties\".\n  # We need a dictionary to translate coordinates into county IDs so we can join them later.\n  # dictionary: {(lat, lon) -> county_id}\n  df_weather_station_to_county_mapping = df_weather_station_to_county_mapping[\n      df_weather_station_to_county_mapping.notnull().all(axis=1)\n    ].sort_values(by=\"county\")\n  result_dict = dict(zip(\n      zip(\n        round(df_weather_station_to_county_mapping['latitude'],1),\n        round(df_weather_station_to_county_mapping['longitude'],1)),\n      df_weather_station_to_county_mapping['county']))\n  df_historical_weather = df_historical_weather[df_historical_weather['datetime'] >= train_start]\n\n  # ---------------------- client data -----------------------\n  df_client_hourly = upsample_daily_to_hourly(df_client, date_col='date')\n  if 'data_block_id' in df_client_hourly.columns:\n    df_client_hourly.drop(columns=['data_block_id'], inplace=True)\n\n  key = [\"county\", \"product_type\", \"is_business\", \"datetime\"]\n  print(df_client_hourly.duplicated(key).sum())\n\n  # merge into our main dataframe\n  df_data['datetime'] = pd.to_datetime(df_data['datetime'])\n  df_data = df_data.merge(df_client_hourly, on=['county','product_type','is_business','datetime'], how='left')\n\n  print(dbg(\"after client merge\", df_data))\n\n  #### CHANGE HERE THREE ROWS ####\n  # ---- client missing indicators + fill ----\n  for col in [\"eic_count\", \"installed_capacity\"]:\n    df_data[f\"{col}_missing\"] = df_data[col].isna().astype(\"int8\")\n    df_data[col] = df_data[col].fillna(0.0)\n\n  # Filter data to ensure we don't go past the available client data\n  client_end_date = df_client_hourly['datetime'].max()\n  df_data = df_data[df_data['datetime'] <= client_end_date]\n\n  # --------------- gas prices -------------------\n  df_gas_hourly = upsample_daily_to_hourly(df_gas_prices, date_col='forecast_date')\n  cols_to_drop = ['origin_date', 'data_block_id']\n  df_gas_hourly.drop(columns=[c for c in cols_to_drop if c in df_gas_hourly.columns], inplace=True)\n\n  gas_end_date = df_gas_hourly['datetime'].max()\n  df_data = df_data[df_data['datetime'] <= gas_end_date]\n\n  df_data = df_data.merge(df_gas_hourly, on=['datetime'], how='left')\n\n  print(dbg(\"after gas merge\", df_data))\n\n  #--------------- electricity (already hourly) ------------------------------\n  df_electricity_prices_try = df_electricity_prices.copy(deep=True)\n  if 'origin_date' in df_electricity_prices_try.columns:\n    df_electricity_prices_try.drop(columns=['origin_date'], inplace=True)\n  if 'data_block_id' in df_electricity_prices_try.columns:\n    df_electricity_prices_try.drop(columns=['data_block_id'], inplace=True)\n  df_electricity_prices_try['forecast_date'] = pd.to_datetime(df_electricity_prices_try['forecast_date'])\n  df_electricity_prices_try.rename(columns={\"forecast_date\": \"datetime\"}, inplace=True)\n  df_data = df_data.merge(df_electricity_prices_try, on=['datetime'], how='left')\n\n  print(dbg(\"after elec merge\", df_data))\n\n  # --- forecast weather ---\n  df_forecast_processed = process_forecast_weather(df_forecast_weather, result_dict)\n  df_data = df_data.merge(df_forecast_processed, on=['county', 'datetime'], how='left')\n\n  print(dbg(\"after forecast merge\", df_data))\n\n  # --- process historical weather ---\n  df_weather_processed = process_historical_weather(df_historical_weather, result_dict)\n  df_data = df_data.merge(df_weather_processed, on=['county', 'datetime'], how='left')\n  df_data = add_time_features(df_data, datetime_col=\"datetime\")\n\n  return df_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:55:02.600415Z","iopub.execute_input":"2025-12-14T09:55:02.600838Z","iopub.status.idle":"2025-12-14T09:55:02.721236Z","shell.execute_reply.started":"2025-12-14T09:55:02.600805Z","shell.execute_reply":"2025-12-14T09:55:02.719689Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def add_lag_rolling_features(\n    df: pd.DataFrame,\n    group_cols: list,\n    target_col: str = \"target\",\n    datetime_col: str = \"datetime\",\n    lags: tuple = (1, 24, 48, 168),\n    roll_windows: tuple = (24, 168),\n    add_diff: bool = True\n) -> pd.DataFrame:\n    out = df.copy(deep=True)\n    out[datetime_col] = pd.to_datetime(out[datetime_col])\n    out = out.sort_values(group_cols + [datetime_col])\n\n    g = out.groupby(group_cols, sort=False)[target_col]\n\n    # Lags\n    for L in lags:\n        out[f\"{target_col}_lag_{L}\"] = g.shift(L)\n\n    # Diffs\n    if add_diff:\n        out[f\"{target_col}_diff_1\"] = out[target_col] - out[f\"{target_col}_lag_1\"]\n        if 24 in lags:\n            out[f\"{target_col}_diff_24\"] = out[target_col] - out[f\"{target_col}_lag_24\"]\n\n    # Rolling stats on past values (per group, no leakage)\n    for W in roll_windows:\n        out[f\"{target_col}_roll_mean_{W}\"] = g.shift(1).transform(\n            lambda s: s.rolling(W, min_periods=max(3, W//10)).mean()\n        )\n        out[f\"{target_col}_roll_std_{W}\"] = g.shift(1).transform(\n            lambda s: s.rolling(W, min_periods=max(3, W//10)).std()\n        )\n\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:55:02.722944Z","iopub.execute_input":"2025-12-14T09:55:02.723341Z","iopub.status.idle":"2025-12-14T09:55:02.747600Z","shell.execute_reply.started":"2025-12-14T09:55:02.723309Z","shell.execute_reply":"2025-12-14T09:55:02.745945Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"combined_df = generate_features(\n    df_data,\n    df_client,\n    df_gas_prices,\n    df_electricity_prices,\n    df_historical_weather,\n    df_forecast_weather,\n    df_weather_station_to_county_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:55:02.749497Z","iopub.execute_input":"2025-12-14T09:55:02.750100Z","iopub.status.idle":"2025-12-14T09:55:23.745472Z","shell.execute_reply.started":"2025-12-14T09:55:02.750045Z","shell.execute_reply":"2025-12-14T09:55:23.743836Z"}},"outputs":[{"name":"stdout","text":"start: 2,018,352 rows\nNone\n0\nafter client merge: 2,018,352 rows\nNone\nafter gas merge: 2,012,112 rows\nNone\nafter elec merge: 2,012,112 rows\nNone\nafter forecast merge: 2,012,112 rows\nNone\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"  # ---- lags & rolling stats ----\ncombined_df = add_lag_rolling_features(\n    combined_df,\n    group_cols=[\"county\", \"product_type\", \"is_business\"],\n    target_col=\"target\",\n    datetime_col=\"datetime\",\n    lags=(1, 24, 48, 168),\n    roll_windows=(24, 168),\n    add_diff=True\n  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:55:23.747122Z","iopub.execute_input":"2025-12-14T09:55:23.747522Z","iopub.status.idle":"2025-12-14T09:55:27.045473Z","shell.execute_reply.started":"2025-12-14T09:55:23.747479Z","shell.execute_reply":"2025-12-14T09:55:27.044314Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# last n days as validation\ngap_days = 7\nval_days = 90\nmax_dt = combined_df[\"datetime\"].max()\nval_start = max_dt - pd.Timedelta(days=val_days)\ngap_start = val_start - pd.Timedelta(days=gap_days)\n\ntrain_full = combined_df[combined_df[\"datetime\"] < gap_start].copy()\nvalid_full = combined_df[combined_df[\"datetime\"] >= val_start].copy()\n\nprint(\"Train:\", train_full[\"datetime\"].min(), \"→\", train_full[\"datetime\"].max(), len(train_full))\nprint(\"Valid:\", valid_full[\"datetime\"].min(), \"→\", valid_full[\"datetime\"].max(), len(valid_full))\n\ndef make_balanced_subset_regression(\n    df: pd.DataFrame,\n    target_col: str = \"target\",\n    group_cols=(\"prediction_unit_id\", \"is_consumption\"),\n    n_bins: int = 20,\n    frac: float = 0.10,\n    max_rows: int | None = 300_000,\n    random_state: int = 343,\n) -> pd.DataFrame:\n    rng = np.random.default_rng(random_state)\n    parts = []\n\n    if max_rows is not None:\n        frac = min(frac, max_rows / max(len(df), 1))\n\n    for _, gdf in df.groupby(list(group_cols), sort=False):\n        if len(gdf) < 100:\n            parts.append(gdf)\n            continue\n\n        n_take = int(np.ceil(len(gdf) * frac))\n        n_take = max(20, min(n_take, len(gdf)))\n\n        y = gdf[target_col]\n        try:\n            y_binned = pd.qcut(y, q=min(n_bins, len(gdf)), duplicates=\"drop\")\n            if y_binned.nunique() < 2:\n                idx = rng.choice(gdf.index.to_numpy(), size=n_take, replace=False)\n                parts.append(gdf.loc[idx])\n                continue\n\n            sss = StratifiedShuffleSplit(n_splits=1, train_size=n_take, random_state=random_state)\n            idx_take, _ = next(sss.split(np.zeros(len(gdf)), y_binned))\n            parts.append(gdf.iloc[idx_take])\n\n        except Exception:\n            idx = rng.choice(gdf.index.to_numpy(), size=n_take, replace=False)\n            parts.append(gdf.loc[idx])\n\n    out = pd.concat(parts, axis=0)\n\n    if max_rows is not None and len(out) > max_rows:\n        out = out.sample(n=max_rows, random_state=random_state)\n\n    return out.sort_values([\"prediction_unit_id\",\"is_consumption\",\"datetime\"]).reset_index(drop=True)\n\ntrain_sub = make_balanced_subset_regression(\n    train_full,\n    frac=0.10,\n    max_rows=300_000,\n    n_bins=20,\n    random_state=343\n)\n\nprint(\"Balanced train subset rows:\", len(train_sub))\n\n# X and y for training\nDROP_COLS = [\n    \"target\", \"row_id\", \"datetime\"\n]\n\n#KEEP_COLS = [\n#    \"county\", \"product_type\", \"is_business\", \"is_consumption\", \"hour_sin\", \"hour_cos\", \"month_sin, month_cos\",\n#    \"euros_per_mwh\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"\n#]\n\nfeature_cols = [c for c in combined_df.columns if c not in DROP_COLS]\n#feature_cols = [c for c in combined_df.columns if c in KEEP_COLS]\n\n\nX_train = train_sub[feature_cols]\ny_train = train_sub[\"target\"]\n\nX_valid = valid_full[feature_cols]\ny_valid = valid_full[\"target\"]\n\nprint(\"X_train:\", X_train.shape, \"X_valid:\", X_valid.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:55:27.047276Z","iopub.execute_input":"2025-12-14T09:55:27.047725Z","iopub.status.idle":"2025-12-14T09:55:30.487128Z","shell.execute_reply.started":"2025-12-14T09:55:27.047684Z","shell.execute_reply":"2025-12-14T09:55:30.485925Z"}},"outputs":[{"name":"stdout","text":"Train: 2021-09-01 00:00:00 → 2023-02-21 22:00:00 1704108\nValid: 2023-02-28 23:00:00 → 2023-05-29 23:00:00 285588\nBalanced train subset rows: 170480\nX_train: (170480, 70) X_valid: (285588, 70)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#DROP_COLS = [\n#    \"target\", \"row_id\", \"datetime\"\n#]\n\n#feature_cols = [c for c in combined_df.columns if c not in DROP_COLS]\n\n#X_train = combined_df[feature_cols]\n#y_train = combined_df[\"target\"]\n\nmask = y_train.notna()\nX_train = X_train.loc[mask]\ny_train = y_train.loc[mask]\n\nxgbc = xgb.XGBRegressor()\nfitted_xgbc = xgbc.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:55:30.488644Z","iopub.execute_input":"2025-12-14T09:55:30.489060Z","iopub.status.idle":"2025-12-14T09:57:16.403560Z","shell.execute_reply.started":"2025-12-14T09:55:30.489023Z","shell.execute_reply":"2025-12-14T09:57:16.402438Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# series key -> last seen target\nlast_y = {}\nprinted = False\n\ndef update_state(revealed_targets: pd.DataFrame):\n    if revealed_targets is None or len(revealed_targets) == 0:\n        return\n\n    rt = revealed_targets.copy()\n    rt[\"is_business\"] = rt[\"is_business\"].astype(int)\n    rt[\"is_consumption\"] = rt[\"is_consumption\"].astype(int)\n\n    for r in rt.itertuples(index=False):\n        key = (r.county, r.product_type, r.is_business, r.is_consumption)\n        last_y[key] = float(r.target)\n\ndef add_lag1_from_state(df: pd.DataFrame) -> pd.DataFrame:\n    out = df.copy()\n    out[\"is_business\"] = out[\"is_business\"].astype(int)\n    out[\"is_consumption\"] = out[\"is_consumption\"].astype(int)\n\n    lag1 = []\n    for r in out.itertuples(index=False):\n        key = (r.county, r.product_type, r.is_business, r.is_consumption)\n        lag1.append(last_y.get(key, np.nan))\n\n    out[\"target_lag_1\"] = lag1\n    out[\"target_lag_1_missing\"] = out[\"target_lag_1\"].isna().astype(\"int8\")\n    out[\"target_lag_1\"] = out[\"target_lag_1\"].fillna(0.0)\n    return out\n\ndef make_prediction_with_features(\n    test, revealed_targets, client, historical_weather,\n    forecast_weather, electricity_prices, gas_prices,\n    sample_prediction, station_map, model, feature_cols\n):\n    global printed\n\n    update_state(revealed_targets)\n\n    feat = generate_features(\n        df_data=test.copy(),\n        df_client=client,\n        df_gas_prices=gas_prices,\n        df_electricity_prices=electricity_prices,\n        df_historical_weather=historical_weather,\n        df_forecast_weather=forecast_weather,\n        df_weather_station_to_county_mapping=station_map\n    )\n\n    feat = add_lag1_from_state(feat)\n\n    # ---- DEBUG ONCE ----\n    if not printed:\n        print(\"test rows:\", len(test), \"sample_prediction rows:\", len(sample_prediction), \"feat rows:\", len(feat))\n        print(\"test cols:\", list(test.columns))\n        print(\"sample_prediction cols:\", list(sample_prediction.columns))\n        printed = True\n\n    # ---- ALIGN ROWS TO SAMPLE_PREDICTION ----\n    # Enefit normally has row_id. If present, use it to guarantee correct shape/order.\n    if \"row_id\" in sample_prediction.columns and \"row_id\" in feat.columns:\n        feat_aligned = sample_prediction[[\"row_id\"]].merge(feat, on=\"row_id\", how=\"left\")\n    else:\n        # fallback: assume same order/length as test; reindex to sample_prediction index\n        feat_aligned = feat.reindex(sample_prediction.index)\n\n    X_test = feat_aligned.reindex(columns=feature_cols, fill_value=np.nan)\n\n    return X_test\n\nall_preds = []\ncounter = 0\nfor (test, revealed_targets, client, historical_weather,\n        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n\n    test = test.rename(columns={'prediction_datetime':'datetime'})\n    X_test = make_prediction_with_features(\n        test, revealed_targets, client, historical_weather,\n        forecast_weather, electricity_prices, gas_prices,\n        sample_prediction,\n        station_map=df_weather_station_to_county_mapping,   # your mapping df\n        model=fitted_xgbc,                                  # your trained model\n        feature_cols=feature_cols\n    )\n    sample_prediction['target'] = fitted_xgbc.predict(X_test)\n    env.predict(sample_prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T09:57:16.406625Z","iopub.execute_input":"2025-12-14T09:57:16.407069Z","iopub.status.idle":"2025-12-14T09:57:17.214778Z","shell.execute_reply.started":"2025-12-14T09:57:16.407038Z","shell.execute_reply":"2025-12-14T09:57:17.213206Z"}},"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\nstart: 3,120 rows\nNone\n0\nafter client merge: 3,120 rows\nNone\nafter gas merge: 0 rows\nNone\nafter elec merge: 0 rows\nNone\nafter forecast merge: 0 rows\nNone\ntest rows: 3120 sample_prediction rows: 3120 feat rows: 0\ntest cols: ['county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id', 'prediction_unit_id', 'currently_scored']\nsample_prediction cols: ['row_id', 'target']\nstart: 3,120 rows\nNone\n0\nafter client merge: 3,120 rows\nNone\nafter gas merge: 0 rows\nNone\nafter elec merge: 0 rows\nNone\nafter forecast merge: 0 rows\nNone\nstart: 3,120 rows\nNone\n0\nafter client merge: 3,120 rows\nNone\nafter gas merge: 0 rows\nNone\nafter elec merge: 0 rows\nNone\nafter forecast merge: 0 rows\nNone\nstart: 3,120 rows\nNone\n0\nafter client merge: 3,120 rows\nNone\nafter gas merge: 0 rows\nNone\nafter elec merge: 0 rows\nNone\nafter forecast merge: 0 rows\nNone\n","output_type":"stream"}],"execution_count":10}]}
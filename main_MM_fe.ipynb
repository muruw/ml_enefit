{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "X7ce-iZHYoKe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#DATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\n",
        "DATA_DIR = \"./\"\n",
        "\n",
        "df_data = pd.read_csv(DATA_DIR + \"train.csv\")\n",
        "df_client = pd.read_csv(DATA_DIR + \"client.csv\")\n",
        "df_historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
        "df_forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
        "df_electricity_prices = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
        "df_gas_prices = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\n",
        "df_weather_station_to_county_mapping = pd.read_csv(DATA_DIR + \"weather_station_to_county_mapping.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "vlMgKgPXZRu7",
        "outputId": "5dfd47a1-357f-4057-e9cb-62ead0697c09"
      },
      "outputs": [],
      "source": [
        "def dbg(name, df):\n",
        "    print(f\"{name}: {len(df):,} rows\")\n",
        "\n",
        "def upsample_daily_to_hourly(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
        "    # unsample (process) client data. Client data is reported daily, but we need it hourly.\n",
        "    # So we take ever row (1 day) and duplicate (explode) it into 24 rows\n",
        "    df_hourly = df.copy(deep=True) # deep copy to not make changes to dataframe from parameter\n",
        "    df_hourly[date_col] = pd.to_datetime(df_hourly[date_col])\n",
        "\n",
        "    # create a column of lists, where each list contains [00:00, 01:00, ... 23:00] for that day\n",
        "    df_hourly['datetime'] = df_hourly[date_col].apply(lambda x: [x + pd.Timedelta(hours=i) for i in range(24)])\n",
        "    df_hourly = df_hourly.explode('datetime')\n",
        "\n",
        "    # drop the original daily date column as it's no longer needed\n",
        "    df_hourly = df_hourly.drop(columns=[date_col])\n",
        "    return df_hourly\n",
        "\n",
        "\n",
        "# nb! needs more testing as this method was generated by AI.\n",
        "def process_forecast_weather(df_forecast: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans, aggregates, and pivots forecast weather data.\n",
        "\n",
        "    Logic:\n",
        "    1. Map Lat/Lon to County ID.\n",
        "    2. Convert 'Origin Time' (when forecast was made) to 'Target Time' (when weather happens).\n",
        "    3. Group forecasts into 'batches' (Day 1 forecast vs Day 2 forecast).\n",
        "    4. Average the values per county/hour/batch.\n",
        "    5. Pivot so batches become columns (e.g., temperature_1, temperature_2).\n",
        "    \"\"\"\n",
        "    # Safety Copy\n",
        "    df = df_forecast.copy(deep=True)\n",
        "\n",
        "    # 1. Map Coordinates to County\n",
        "    # We use the dictionary passed in from the main function\n",
        "    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n",
        "\n",
        "    # Filter out valid locations only\n",
        "    df = df[df['county'] != -1]\n",
        "\n",
        "    # 2. Calculate Target Time\n",
        "    # Standardize origin time to 02:00:00 (removes minute/second noise)\n",
        "    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'])\n",
        "    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'].dt.date.astype(str) + ' 02:00:00')\n",
        "\n",
        "    # Target Time = Origin + Hours Ahead\n",
        "    df['forecast_datetime'] = df['origin_datetime'] + pd.to_timedelta(df['hours_ahead'], unit='h')\n",
        "\n",
        "    # We don't need origin time anymore\n",
        "    df.drop(columns=['origin_datetime'], inplace=True)\n",
        "\n",
        "    # 3. Create \"Batches\" (cumcount) todo:rename\n",
        "    # A batch represents how far out the forecast is (Day 1 vs Day 2)\n",
        "    # (hours_ahead - 1) // 24 + 1 results in: 1 for 0-24h, 2 for 25-48h\n",
        "    df['cumcount'] = (df['hours_ahead'] - 1) // 24 + 1\n",
        "\n",
        "    # 4. Aggregate (Mean) by County, Time, and Batch\n",
        "    # Identify feature columns (exclude IDs and Time)\n",
        "    exclude_cols = ['latitude', 'longitude', 'hours_ahead', 'forecast_datetime', 'cumcount', 'county', 'data_block_id']\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    agg_dict = {col: 'mean' for col in feature_cols}\n",
        "    # We must keep 'cumcount' in the groupby keys, so we don't aggregate it\n",
        "\n",
        "    df_grouped = df.groupby(['county', 'forecast_datetime', 'cumcount']).agg(agg_dict)\n",
        "\n",
        "    # 5. Pivot (Unstack)\n",
        "    # Moves 'cumcount' from a row index to a column suffix\n",
        "    df_pivoted = df_grouped.unstack(level=-1)\n",
        "\n",
        "    # Flatten MultiIndex columns: ('temperature', 1) -> 'temperature_1'\n",
        "    df_pivoted.columns = [f'{col[0]}_{col[1]}' for col in df_pivoted.columns]\n",
        "\n",
        "    df_pivoted.reset_index(inplace=True)\n",
        "    df_pivoted.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
        "\n",
        "    # Handle missing values (if a forecast is missing, fill with 0 or strictly manage it)\n",
        "    df_pivoted.fillna(0, inplace=True)\n",
        "\n",
        "    return df_pivoted\n",
        "\n",
        "def process_historical_weather(df_historical: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n",
        "    #compose historical weather data (averaging stations per county).\n",
        "    df = df_historical.copy(deep=True)\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n",
        "    df = df[df['county'] != -1]\n",
        "\n",
        "    exclude_cols = ['latitude', 'longitude', 'datetime', 'county', 'data_block_id']\n",
        "    agg_dict = {col: 'mean' for col in df.columns if col not in exclude_cols}\n",
        "    df_grouped = df.groupby(['county', 'datetime']).agg(agg_dict)\n",
        "    df_grouped.reset_index(inplace=True)\n",
        "\n",
        "    return df_grouped\n",
        "\n",
        "\n",
        "  #### CHANGE HERE NEW FUNCTION ####\n",
        "def add_time_features(df: pd.DataFrame, datetime_col: str = \"datetime\") -> pd.DataFrame:\n",
        "    out = df.copy(deep=True)\n",
        "    dt = pd.to_datetime(out[datetime_col])\n",
        "\n",
        "    # Basic calendar parts\n",
        "    out[\"hour\"] = dt.dt.hour.astype(\"int8\")\n",
        "    out[\"dayofweek\"] = dt.dt.dayofweek.astype(\"int8\")   \n",
        "    out[\"month\"] = dt.dt.month.astype(\"int8\")          \n",
        "    out[\"day\"] = dt.dt.day.astype(\"int8\")          \n",
        "    out[\"is_weekend\"] = (out[\"dayofweek\"] >= 5).astype(\"int8\")\n",
        "\n",
        "    # Cyclical encodings\n",
        "    out[\"hour_sin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
        "    out[\"hour_cos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
        "\n",
        "    out[\"month_sin\"] = np.sin(2 * np.pi * out[\"month\"] / 12)\n",
        "    out[\"month_cos\"] = np.cos(2 * np.pi * out[\"month\"] / 12)\n",
        "\n",
        "    return out\n",
        "\n",
        "def generate_features(\n",
        "    df_data: pd.DataFrame,\n",
        "    df_client: pd.DataFrame,\n",
        "    df_gas_prices: pd.DataFrame,\n",
        "    df_electricity_prices: pd.DataFrame,\n",
        "    df_historical_weather: pd.DataFrame,\n",
        "    df_forecast_weather: pd.DataFrame,\n",
        "    df_weather_station_to_county_mapping: pd.DataFrame,\n",
        "    train_start = '2021-09-01 11:00:00'):\n",
        "  \n",
        "  print(dbg(\"start\", df_data))\n",
        "\n",
        "  # The weather data uses Latitude/Longitude, but the energy data uses \"Counties\".\n",
        "  # We need a dictionary to translate coordinates into county IDs so we can join them later.\n",
        "  # dictionary: {(lat, lon) -> county_id}\n",
        "  df_weather_station_to_county_mapping = df_weather_station_to_county_mapping[\n",
        "      df_weather_station_to_county_mapping.notnull().all(axis=1)\n",
        "    ].sort_values(by=\"county\")\n",
        "  result_dict = dict(zip(\n",
        "      zip(\n",
        "        round(df_weather_station_to_county_mapping['latitude'],1),\n",
        "        round(df_weather_station_to_county_mapping['longitude'],1)),\n",
        "      df_weather_station_to_county_mapping['county']))\n",
        "  df_historical_weather = df_historical_weather[df_historical_weather['datetime'] >= train_start]\n",
        "\n",
        "  # ---------------------- client data -----------------------\n",
        "  df_client_hourly = upsample_daily_to_hourly(df_client, date_col='date')\n",
        "  if 'data_block_id' in df_client_hourly.columns:\n",
        "    df_client_hourly.drop(columns=['data_block_id'], inplace=True)\n",
        "\n",
        "  key = [\"county\", \"product_type\", \"is_business\", \"datetime\"]\n",
        "  print(df_client_hourly.duplicated(key).sum())\n",
        "\n",
        "  # merge into our main dataframe\n",
        "  df_data['datetime'] = pd.to_datetime(df_data['datetime'])\n",
        "  df_data = df_data.merge(df_client_hourly, on=['county','product_type','is_business','datetime'], how='left')\n",
        "\n",
        "  print(dbg(\"after client merge\", df_data))\n",
        "\n",
        "  #### CHANGE HERE THREE ROWS ####\n",
        "  # ---- client missing indicators + fill ----\n",
        "  for col in [\"eic_count\", \"installed_capacity\"]:\n",
        "    df_data[f\"{col}_missing\"] = df_data[col].isna().astype(\"int8\")\n",
        "    df_data[col] = df_data[col].fillna(0.0)\n",
        "\n",
        "  # Filter data to ensure we don't go past the available client data\n",
        "  client_end_date = df_client_hourly['datetime'].max()\n",
        "  df_data = df_data[df_data['datetime'] <= client_end_date]\n",
        "\n",
        "  # --------------- gas prices -------------------\n",
        "  df_gas_hourly = upsample_daily_to_hourly(df_gas_prices, date_col='forecast_date')\n",
        "  cols_to_drop = ['origin_date', 'data_block_id']\n",
        "  df_gas_hourly.drop(columns=[c for c in cols_to_drop if c in df_gas_hourly.columns], inplace=True)\n",
        "\n",
        "  gas_end_date = df_gas_hourly['datetime'].max()\n",
        "  df_data = df_data[df_data['datetime'] <= gas_end_date]\n",
        "\n",
        "  df_data = df_data.merge(df_gas_hourly, on=['datetime'], how='left')\n",
        "\n",
        "  print(dbg(\"after gas merge\", df_data))\n",
        "\n",
        "  #--------------- electricity (already hourly) ------------------------------\n",
        "  df_electricity_prices_try = df_electricity_prices.copy(deep=True)\n",
        "  if 'origin_date' in df_electricity_prices_try.columns:\n",
        "    df_electricity_prices_try.drop(columns=['origin_date'], inplace=True)\n",
        "  if 'data_block_id' in df_electricity_prices_try.columns:\n",
        "    df_electricity_prices_try.drop(columns=['data_block_id'], inplace=True)\n",
        "  df_electricity_prices_try['forecast_date'] = pd.to_datetime(df_electricity_prices_try['forecast_date'])\n",
        "  df_electricity_prices_try.rename(columns={\"forecast_date\": \"datetime\"}, inplace=True)\n",
        "  df_data = df_data.merge(df_electricity_prices_try, on=['datetime'], how='left')\n",
        "\n",
        "  print(dbg(\"after elec merge\", df_data))\n",
        "\n",
        "  # --- forecast weather ---\n",
        "  df_forecast_processed = process_forecast_weather(df_forecast_weather, result_dict)\n",
        "  df_data = df_data.merge(df_forecast_processed, on=['county', 'datetime'], how='left')\n",
        "\n",
        "  print(dbg(\"after forecast merge\", df_data))\n",
        "\n",
        "  # --- process historical weather ---\n",
        "  df_weather_processed = process_historical_weather(df_historical_weather, result_dict)\n",
        "  df_data = df_data.merge(df_weather_processed, on=['county', 'datetime'], how='left')\n",
        "  df_data = add_time_features(df_data, datetime_col=\"datetime\")\n",
        "\n",
        "  return df_data\n",
        "\n",
        "# ---- lags & rolling stats ----\n",
        "def add_lag_rolling_features(\n",
        "    df: pd.DataFrame,\n",
        "    group_cols: list,\n",
        "    target_col: str = \"target\",\n",
        "    datetime_col: str = \"datetime\",\n",
        "    lags: tuple = (1, 24, 48, 168),\n",
        "    roll_windows: tuple = (24, 168),\n",
        "    add_diff: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    out = df.copy(deep=True)\n",
        "    out[datetime_col] = pd.to_datetime(out[datetime_col])\n",
        "    out = out.sort_values(group_cols + [datetime_col])\n",
        "\n",
        "    g = out.groupby(group_cols, sort=False)[target_col]\n",
        "\n",
        "    # --- Lags ---\n",
        "    for L in lags:\n",
        "        out[f\"{target_col}_lag_{L}\"] = g.shift(L)\n",
        "\n",
        "    # --- Diffs ---\n",
        "    if add_diff:\n",
        "        out[f\"{target_col}_diff_1\"] = out[target_col] - out[f\"{target_col}_lag_1\"]\n",
        "        if 24 in lags:\n",
        "            out[f\"{target_col}_diff_24\"] = out[target_col] - out[f\"{target_col}_lag_24\"]\n",
        "\n",
        "    # --- Rolling stats on past values (shifted by 1 to avoid leakage) ---\n",
        "    for W in roll_windows:\n",
        "        out[f\"{target_col}_roll_mean_{W}\"] = g.shift(1).rolling(W, min_periods=max(3, W//10)).mean()\n",
        "        out[f\"{target_col}_roll_std_{W}\"]  = g.shift(1).rolling(W, min_periods=max(3, W//10)).std()\n",
        "\n",
        "    return out\n",
        "\n",
        "  # ---- lags & rolling stats ----\n",
        "df_data = add_lag_rolling_features(\n",
        "    df_data,\n",
        "    group_cols=[\"county\", \"product_type\", \"is_business\"],\n",
        "    target_col=\"target\",\n",
        "    datetime_col=\"datetime\",\n",
        "    lags=(1, 24, 48, 168),\n",
        "    roll_windows=(24, 168),\n",
        "    add_diff=True\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cz4cU4KG2TiW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start: 2,018,352 rows\n",
            "None\n",
            "0\n",
            "after client merge: 2,018,352 rows\n",
            "None\n",
            "after gas merge: 2,012,112 rows\n",
            "None\n",
            "after elec merge: 2,012,112 rows\n",
            "None\n",
            "after forecast merge: 2,012,112 rows\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "combined_df = generate_features(\n",
        "    df_data,\n",
        "    df_client,\n",
        "    df_gas_prices,\n",
        "    df_electricity_prices,\n",
        "    df_historical_weather,\n",
        "    df_forecast_weather,\n",
        "    df_weather_station_to_county_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "AvBjfoKj2zrB",
        "outputId": "c14d17bf-1e45-4118-8429-29b37091bcae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>county</th>\n",
              "      <th>is_business</th>\n",
              "      <th>product_type</th>\n",
              "      <th>target</th>\n",
              "      <th>is_consumption</th>\n",
              "      <th>datetime</th>\n",
              "      <th>data_block_id</th>\n",
              "      <th>row_id</th>\n",
              "      <th>prediction_unit_id</th>\n",
              "      <th>eic_count</th>\n",
              "      <th>...</th>\n",
              "      <th>diffuse_radiation</th>\n",
              "      <th>hour</th>\n",
              "      <th>dayofweek</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>hour_sin</th>\n",
              "      <th>hour_cos</th>\n",
              "      <th>month_sin</th>\n",
              "      <th>month_cos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>96.590</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>17.314</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.904</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>688.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.836970e-16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 63 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   county  is_business  product_type  target  is_consumption   datetime  \\\n",
              "0       0            0             1   0.713               0 2021-09-01   \n",
              "1       0            0             1  96.590               1 2021-09-01   \n",
              "2       0            0             2   0.000               0 2021-09-01   \n",
              "3       0            0             2  17.314               1 2021-09-01   \n",
              "4       0            0             3   2.904               0 2021-09-01   \n",
              "\n",
              "   data_block_id  row_id  prediction_unit_id  eic_count  ...  \\\n",
              "0              0       0                   0      108.0  ...   \n",
              "1              0       1                   0      108.0  ...   \n",
              "2              0       2                   1       17.0  ...   \n",
              "3              0       3                   1       17.0  ...   \n",
              "4              0       4                   2      688.0  ...   \n",
              "\n",
              "   diffuse_radiation  hour  dayofweek  month  day  is_weekend  hour_sin  \\\n",
              "0                NaN     0          2      9    1           0       0.0   \n",
              "1                NaN     0          2      9    1           0       0.0   \n",
              "2                NaN     0          2      9    1           0       0.0   \n",
              "3                NaN     0          2      9    1           0       0.0   \n",
              "4                NaN     0          2      9    1           0       0.0   \n",
              "\n",
              "   hour_cos  month_sin     month_cos  \n",
              "0       1.0       -1.0 -1.836970e-16  \n",
              "1       1.0       -1.0 -1.836970e-16  \n",
              "2       1.0       -1.0 -1.836970e-16  \n",
              "3       1.0       -1.0 -1.836970e-16  \n",
              "4       1.0       -1.0 -1.836970e-16  \n",
              "\n",
              "[5 rows x 63 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n",
              "       'datetime', 'data_block_id', 'row_id', 'prediction_unit_id',\n",
              "       'target_lag_1', 'target_lag_24', 'target_lag_48', 'target_lag_168',\n",
              "       'target_diff_1', 'target_diff_24', 'target_roll_mean_24',\n",
              "       'target_roll_std_24', 'target_roll_mean_168', 'target_roll_std_168',\n",
              "       'eic_count', 'installed_capacity', 'eic_count_missing',\n",
              "       'installed_capacity_missing', 'lowest_price_per_mwh',\n",
              "       'highest_price_per_mwh', 'euros_per_mwh', 'temperature_1',\n",
              "       'temperature_2', 'dewpoint_1', 'dewpoint_2', 'cloudcover_high_1',\n",
              "       'cloudcover_high_2', 'cloudcover_low_1', 'cloudcover_low_2',\n",
              "       'cloudcover_mid_1', 'cloudcover_mid_2', 'cloudcover_total_1',\n",
              "       'cloudcover_total_2', '10_metre_u_wind_component_1',\n",
              "       '10_metre_u_wind_component_2', '10_metre_v_wind_component_1',\n",
              "       '10_metre_v_wind_component_2', 'direct_solar_radiation_1',\n",
              "       'direct_solar_radiation_2', 'surface_solar_radiation_downwards_1',\n",
              "       'surface_solar_radiation_downwards_2', 'snowfall_1', 'snowfall_2',\n",
              "       'total_precipitation_1', 'total_precipitation_2', 'temperature',\n",
              "       'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total',\n",
              "       'cloudcover_low', 'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
              "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
              "       'diffuse_radiation', 'hour', 'dayofweek', 'month', 'day', 'is_weekend',\n",
              "       'hour_sin', 'hour_cos', 'month_sin', 'month_cos'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 2021-09-01 00:00:00 → 2023-02-21 22:00:00 1704108\n",
            "Valid: 2023-02-28 23:00:00 → 2023-05-29 23:00:00 285588\n",
            "Balanced train subset rows: 170480\n",
            "X_train: (170480, 70) X_valid: (285588, 70)\n"
          ]
        }
      ],
      "source": [
        "# last n days as validation\n",
        "gap_days = 7\n",
        "val_days = 90\n",
        "max_dt = combined_df[\"datetime\"].max()\n",
        "val_start = max_dt - pd.Timedelta(days=val_days)\n",
        "gap_start = val_start - pd.Timedelta(days=gap_days)\n",
        "\n",
        "train_full = combined_df[combined_df[\"datetime\"] < gap_start].copy()\n",
        "valid_full = combined_df[combined_df[\"datetime\"] >= val_start].copy()\n",
        "\n",
        "print(\"Train:\", train_full[\"datetime\"].min(), \"→\", train_full[\"datetime\"].max(), len(train_full))\n",
        "print(\"Valid:\", valid_full[\"datetime\"].min(), \"→\", valid_full[\"datetime\"].max(), len(valid_full))\n",
        "\n",
        "def make_balanced_subset_regression(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str = \"target\",\n",
        "    group_cols=(\"prediction_unit_id\", \"is_consumption\"),\n",
        "    n_bins: int = 20,\n",
        "    frac: float = 0.10,\n",
        "    max_rows: int | None = 300_000,\n",
        "    random_state: int = 343,\n",
        ") -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    parts = []\n",
        "\n",
        "    if max_rows is not None:\n",
        "        frac = min(frac, max_rows / max(len(df), 1))\n",
        "\n",
        "    for _, gdf in df.groupby(list(group_cols), sort=False):\n",
        "        if len(gdf) < 100:\n",
        "            parts.append(gdf)\n",
        "            continue\n",
        "\n",
        "        n_take = int(np.ceil(len(gdf) * frac))\n",
        "        n_take = max(20, min(n_take, len(gdf)))\n",
        "\n",
        "        y = gdf[target_col]\n",
        "        try:\n",
        "            y_binned = pd.qcut(y, q=min(n_bins, len(gdf)), duplicates=\"drop\")\n",
        "            if y_binned.nunique() < 2:\n",
        "                idx = rng.choice(gdf.index.to_numpy(), size=n_take, replace=False)\n",
        "                parts.append(gdf.loc[idx])\n",
        "                continue\n",
        "\n",
        "            sss = StratifiedShuffleSplit(n_splits=1, train_size=n_take, random_state=random_state)\n",
        "            idx_take, _ = next(sss.split(np.zeros(len(gdf)), y_binned))\n",
        "            parts.append(gdf.iloc[idx_take])\n",
        "\n",
        "        except Exception:\n",
        "            idx = rng.choice(gdf.index.to_numpy(), size=n_take, replace=False)\n",
        "            parts.append(gdf.loc[idx])\n",
        "\n",
        "    out = pd.concat(parts, axis=0)\n",
        "\n",
        "    if max_rows is not None and len(out) > max_rows:\n",
        "        out = out.sample(n=max_rows, random_state=random_state)\n",
        "\n",
        "    return out.sort_values([\"prediction_unit_id\",\"is_consumption\",\"datetime\"]).reset_index(drop=True)\n",
        "\n",
        "train_sub = make_balanced_subset_regression(\n",
        "    train_full,\n",
        "    frac=0.10,\n",
        "    max_rows=300_000,\n",
        "    n_bins=20,\n",
        "    random_state=343\n",
        ")\n",
        "\n",
        "print(\"Balanced train subset rows:\", len(train_sub))\n",
        "\n",
        "# X and y for training\n",
        "DROP_COLS = [\n",
        "    \"target\", \"row_id\", \"datetime\"\n",
        "]\n",
        "\n",
        "#KEEP_COLS = [\n",
        "#    \"county\", \"product_type\", \"is_business\", \"is_consumption\", \"hour_sin\", \"hour_cos\", \"month_sin, month_cos\",\n",
        "#    \"euros_per_mwh\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"\n",
        "#]\n",
        "\n",
        "feature_cols = [c for c in combined_df.columns if c not in DROP_COLS]\n",
        "#feature_cols = [c for c in combined_df.columns if c in KEEP_COLS]\n",
        "\n",
        "\n",
        "X_train = train_sub[feature_cols]\n",
        "y_train = train_sub[\"target\"]\n",
        "\n",
        "X_valid = valid_full[feature_cols]\n",
        "y_valid = valid_full[\"target\"]\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"X_valid:\", X_valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "mask = y_train.notna()\n",
        "\n",
        "X_train = X_train.loc[mask]\n",
        "y_train = y_train.loc[mask]\n",
        "\n",
        "regr = RandomForestRegressor(\n",
        "    n_estimators = 300,\n",
        "    min_samples_leaf = 50,\n",
        "    max_features = \"sqrt\",\n",
        "    random_state = 343,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "fitted_regr = regr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "57.415047849489916"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mask = y_valid.notna()\n",
        "\n",
        "X_valid_mask = X_valid.loc[mask]\n",
        "y_valid_mask = y_valid.loc[mask]\n",
        "\n",
        "\n",
        "y_pred_valid = fitted_regr.predict(X_valid_mask)\n",
        "rmse = mean_absolute_error(y_valid_mask, y_pred_valid)\n",
        "rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 24.89680187082571\n",
            "Val:   54.82713945353171\n"
          ]
        }
      ],
      "source": [
        "cv = cross_validate(\n",
        "    regr,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv = 3,\n",
        "    scoring = \"neg_mean_absolute_error\",\n",
        "    return_train_score = True\n",
        ")\n",
        "\n",
        "print(\"Train:\", -cv[\"train_score\"].mean())\n",
        "print(\"Val:  \", -cv[\"test_score\"].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23.53427545773242"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgbc = xgb.XGBRegressor()\n",
        "fitted_xgbc = xgbc.fit(X_train, y_train)\n",
        "\n",
        "y_pred_valid = fitted_xgbc.predict(X_valid_mask)\n",
        "rmse = mean_absolute_error(y_valid_mask, y_pred_valid)\n",
        "rmse"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

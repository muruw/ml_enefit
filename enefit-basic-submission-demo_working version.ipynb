{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Combines main2.ipynb feature engineering with Kaggle API template\n\n\n\"\"\"\nThis notebook includes:\n1. All helper functions from main2.ipynb\n2. Complete feature engineering pipeline\n3. XGBoost model training with all features\n4. Kaggle API prediction loop with state management\n\nUSE: Copy entire code into ONE cell in Kaggle notebook\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom collections import deque\nimport enefit\n\n# HELPER FUNCTIONS \n\ndef upsample_daily_to_hourly(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n    \"\"\"\n    Upsample (process) client data. Client data is reported daily, but we need it hourly.\n    Takes every row (1 day) and duplicates (explodes) it into 24 rows.\n    \"\"\"\n    df_hourly = df.copy(deep=True)\n    df_hourly[date_col] = pd.to_datetime(df_hourly[date_col])\n    df_hourly['datetime'] = df_hourly[date_col].apply(\n        lambda x: [x + pd.Timedelta(hours=i) for i in range(24)]\n    )\n    df_hourly = df_hourly.explode('datetime')\n    df_hourly = df_hourly.drop(columns=[date_col])\n    return df_hourly\n\n\ndef process_forecast_weather(df_forecast: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n    \"\"\"\n    Cleans, aggregates, and pivots forecast weather data.\n    \n    Logic:\n    1. Map Lat/Lon to County ID\n    2. Convert 'Origin Time' (when forecast was made) to 'Target Time' (when weather happens)\n    3. Group forecasts into 'batches' (Day 1 forecast vs Day 2 forecast)\n    4. Average the values per county/hour/batch\n    5. Pivot so batches become columns (e.g., temperature_1, temperature_2)\n    \"\"\"\n    df = df_forecast.copy(deep=True)\n    \n    # 1. Map Coordinates to County\n    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n    df = df[df['county'] != -1]\n    \n    # 2. Calculate Target Time\n    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'])\n    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'].dt.date.astype(str) + ' 02:00:00')\n    df['forecast_datetime'] = df['origin_datetime'] + pd.to_timedelta(df['hours_ahead'], unit='h')\n    df.drop(columns=['origin_datetime'], inplace=True)\n    \n    # 3. Create \"Batches\" (cumcount)\n    df['cumcount'] = (df['hours_ahead'] - 1) // 24 + 1\n    \n    # 4. Aggregate (Mean) by County, Time, and Batch\n    exclude_cols = ['latitude', 'longitude', 'hours_ahead', 'forecast_datetime', \n                    'cumcount', 'county', 'data_block_id']\n    feature_cols = [col for col in df.columns if col not in exclude_cols]\n    agg_dict = {col: 'mean' for col in feature_cols}\n    \n    df_grouped = df.groupby(['county', 'forecast_datetime', 'cumcount']).agg(agg_dict)\n    \n    # 5. Pivot Batches into Columns\n    df_pivoted = df_grouped.unstack(level=-1)\n    df_pivoted.columns = [f'{col[0]}_{col[1]}' for col in df_pivoted.columns]\n    df_pivoted.reset_index(inplace=True)\n    df_pivoted.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n    df_pivoted.fillna(0, inplace=True)\n    \n    return df_pivoted\n\n\ndef process_historical_weather(df_historical: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n    \"\"\"\n    Process historical weather by averaging multiple weather stations per county.\n    \"\"\"\n    df = df_historical.copy(deep=True)\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    \n    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n    df = df[df['county'] != -1]\n    \n    exclude_cols = ['latitude', 'longitude', 'datetime', 'county', 'data_block_id']\n    agg_dict = {col: 'mean' for col in df.columns if col not in exclude_cols}\n    \n    df_grouped = df.groupby(['county', 'datetime']).agg(agg_dict)\n    df_grouped.reset_index(inplace=True)\n    \n    return df_grouped\n\n\ndef generate_features(\n    df_data,\n    df_client,\n    df_gas_prices,\n    df_electricity_prices,\n    df_historical_weather,\n    df_forecast_weather,\n    df_weather_station_to_county_mapping\n):\n    \"\"\"\n    Generate all features by merging multiple data sources.\n    \n    Merges:\n    - Client data (eic_count, installed_capacity)\n    - Gas prices (lowest, highest)\n    - Electricity prices (euros_per_mwh)\n    - Historical weather\n    - Forecast weather (with Day 1/Day 2 batches)\n    \"\"\"\n    # Build location mapping\n    location_map = dict(zip(\n        zip(\n            round(df_weather_station_to_county_mapping['latitude'], 1),\n            round(df_weather_station_to_county_mapping['longitude'], 1)\n        ),\n        df_weather_station_to_county_mapping['county']\n    ))\n    \n    combined_df = df_data.copy()\n    combined_df['datetime'] = pd.to_datetime(combined_df['datetime'])\n    \n    # Merge client data (daily → hourly)\n    if len(df_client) > 0:\n        df_client_hourly = upsample_daily_to_hourly(df_client, date_col='date')\n        if 'data_block_id' in df_client_hourly.columns:\n            df_client_hourly.drop(columns=['data_block_id'], inplace=True)\n        combined_df = combined_df.merge(\n            df_client_hourly, \n            on=['county', 'product_type', 'is_business', 'datetime'], \n            how='left'\n        )\n    \n    # Merge gas prices (daily → hourly)\n    if len(df_gas_prices) > 0:\n        df_gas_hourly = upsample_daily_to_hourly(df_gas_prices, date_col='forecast_date')\n        df_gas_hourly.drop(\n            columns=[c for c in ['origin_date', 'data_block_id'] \n                    if c in df_gas_hourly.columns], \n            inplace=True\n        )\n        combined_df = combined_df.merge(df_gas_hourly, on=['datetime'], how='left')\n    \n    # Merge electricity prices\n    if len(df_electricity_prices) > 0:\n        df_elec = df_electricity_prices.copy()\n        df_elec.drop(\n            columns=[c for c in ['origin_date', 'data_block_id'] \n                    if c in df_elec.columns], \n            inplace=True\n        )\n        df_elec['forecast_date'] = pd.to_datetime(df_elec['forecast_date'])\n        df_elec.rename(columns={\"forecast_date\": \"datetime\"}, inplace=True)\n        combined_df = combined_df.merge(df_elec, on=['datetime'], how='left')\n    \n    # Merge forecast weather\n    if len(df_forecast_weather) > 0:\n        forecast_processed = process_forecast_weather(df_forecast_weather, location_map)\n        combined_df = combined_df.merge(\n            forecast_processed, \n            on=['county', 'datetime'], \n            how='left'\n        )\n    \n    # Merge historical weather\n    if len(df_historical_weather) > 0:\n        weather_processed = process_historical_weather(df_historical_weather, location_map)\n        combined_df = combined_df.merge(\n            weather_processed, \n            on=['county', 'datetime'], \n            how='left'\n        )\n    \n    # Fill NaN\n    combined_df = combined_df.fillna(0)\n    \n    return combined_df\n\n\n# LOAD TRAINING DATA\n\nDATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\n\ndf_data = pd.read_csv(DATA_DIR + \"train.csv\")\ndf_client = pd.read_csv(DATA_DIR + \"client.csv\")\ndf_historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\ndf_forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\ndf_electricity_prices = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\ndf_gas_prices = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\ndf_weather_station_to_county_mapping = pd.read_csv(\n    DATA_DIR + \"weather_station_to_county_mapping.csv\"\n)\n\nprint(f\"   Train: {len(df_data):,} rows\")\nprint(f\"   Client: {len(df_client):,} rows\")\nprint(f\"   Historical weather: {len(df_historical_weather):,} rows\")\nprint(f\"   Forecast weather: {len(df_forecast_weather):,} rows\")\nprint(f\"   Electricity prices: {len(df_electricity_prices):,} rows\")\nprint(f\"   Gas prices: {len(df_gas_prices):,} rows\")\n\n\n# FEATURE ENGINEERING\n\nprint(\"\\n1. Generating base features (merging data sources)...\")\ncombined_df = generate_features(\n    df_data,\n    df_client,\n    df_gas_prices,\n    df_electricity_prices,\n    df_historical_weather,\n    df_forecast_weather,\n    df_weather_station_to_county_mapping\n)\nprint(f\"   Combined shape: {combined_df.shape}\")\n\nprint(\"\\n2. Adding time features...\")\ncombined_df['hour'] = pd.to_datetime(combined_df['datetime']).dt.hour\ncombined_df['day_of_week'] = pd.to_datetime(combined_df['datetime']).dt.dayofweek\ncombined_df['month'] = pd.to_datetime(combined_df['datetime']).dt.month\ncombined_df['is_weekend'] = combined_df['day_of_week'].isin([5, 6]).astype(int)\nprint(\"Added: hour, day_of_week, month, is_weekend\")\n\nprint(\"\\n3. Adding lag features...\")\ncombined_df = combined_df.sort_values(['county', 'product_type', 'is_business', 'datetime'])\ncombined_df['target_lag_24h'] = combined_df.groupby(\n    ['county', 'product_type', 'is_business']\n)['target'].shift(24)\ncombined_df['target_lag_168h'] = combined_df.groupby(\n    ['county', 'product_type', 'is_business']\n)['target'].shift(168)\nprint(\"Added: target_lag_24h, target_lag_168h\")\n\n# Define feature list\nimproved_features = [\n    # Identifiers\n    'county', 'is_business', 'product_type', 'is_consumption',\n    # Client info\n    'eic_count', 'installed_capacity',\n    # Prices\n    'euros_per_mwh', 'lowest_price_per_mwh', 'highest_price_per_mwh',\n    # Weather (historical)\n    'temperature', 'cloudcover_total', 'rain',\n    # Weather (forecast)\n    'temperature_1', 'temperature_2',\n    # Time\n    'hour', 'day_of_week', 'month', 'is_weekend',\n    # Lags\n    'target_lag_24h', 'target_lag_168h',\n]\n\n# Filter features that exist\nimproved_features = [f for f in improved_features if f in combined_df.columns]\n\n\n\n# PREPARE TRAINING DATA\ntrain_df = combined_df[combined_df['target'].notna()].copy()\nprint(f\"\\n1. Rows with target: {len(train_df):,}\")\n\ntrain_df = train_df.dropna(subset=['target_lag_24h', 'target_lag_168h'])\nprint(f\"2. After dropping NaN lags: {len(train_df):,}\")\n\n# Sample for faster training (30%)\ntrain_sample = train_df.sample(frac=0.3, random_state=42)\nprint(f\"3. After sampling (30%): {len(train_sample):,}\")\n\nX = train_sample[improved_features].copy()\ny = train_sample['target'].copy()\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(f\"   Training: {len(X_train):,} rows\")\nprint(f\"   Validation: {len(X_val):,} rows\")\n\n\n# TRAIN XGBOOST MODEL\n\nparams = {\n    'objective': 'reg:squarederror',\n    'max_depth': 6,\n    'learning_rate': 0.05,\n    'n_estimators': 150,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 3,\n    'gamma': 0.1,\n    'random_state': 42,\n    'n_jobs': -1,\n    'tree_method': 'hist',\n}\n\nmodel = xgb.XGBRegressor(**params)\nmodel.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n\n# Evaluate\ny_pred_train = model.predict(X_train)\ny_pred_val = model.predict(X_val)\n\ntrain_mae = mean_absolute_error(y_train, y_pred_train)\nval_mae = mean_absolute_error(y_val, y_pred_val)\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\nval_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n\n\nprint(f\"Training MAE:    {train_mae:.2f}\")\nprint(f\"Validation MAE:  {val_mae:.2f}\")\nprint(f\"Training RMSE:   {train_rmse:.2f}\")\nprint(f\"Validation RMSE: {val_rmse:.2f}\")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': improved_features,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nfor idx, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n    print(f\"   {idx:2d}. {row['feature']:30s} : {row['importance']*100:.2f}%\")\n\n\nlast_y = {}\nprinted_debug = False\n\ndef update_state(revealed_targets: pd.DataFrame):\n    \"\"\"Update state with revealed targets for lag features.\"\"\"\n    if revealed_targets is None or len(revealed_targets) == 0:\n        return\n    \n    rt = revealed_targets.copy()\n    rt[\"is_business\"] = rt[\"is_business\"].astype(int)\n    rt[\"is_consumption\"] = rt[\"is_consumption\"].astype(int)\n    \n    for r in rt.itertuples(index=False):\n        key = (r.county, r.product_type, r.is_business, r.is_consumption)\n        last_y[key] = float(r.target)\n\n\ndef add_lag1_from_state(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Add lag_1 feature from state (most recent revealed target).\"\"\"\n    out = df.copy()\n    out[\"is_business\"] = out[\"is_business\"].astype(int)\n    out[\"is_consumption\"] = out[\"is_consumption\"].astype(int)\n    \n    lag1 = []\n    for r in out.itertuples(index=False):\n        key = (r.county, r.product_type, r.is_business, r.is_consumption)\n        lag1.append(last_y.get(key, 0.0))\n    \n    out[\"target_lag_1\"] = lag1\n    return out\n\n\n# Initialize API\nenv = enefit.make_env()\niter_test = env.iter_test()\n\ncounter = 0\n\n# Main prediction loop\nfor (test, revealed_targets, client, historical_weather,\n     forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n    \n    counter += 1\n    \n    if counter % 100 == 0:\n        print(f\" Iteration {counter}...\")\n    \n    # Update state with revealed targets\n    update_state(revealed_targets)\n    \n    # Rename column to match training\n    if 'prediction_datetime' in test.columns:\n        test = test.rename(columns={'prediction_datetime': 'datetime'})\n    \n    # Generate features\n    feat = generate_features(\n        df_data=test.copy(),\n        df_client=client,\n        df_gas_prices=gas_prices,\n        df_electricity_prices=electricity_prices,\n        df_historical_weather=historical_weather,\n        df_forecast_weather=forecast_weather,\n        df_weather_station_to_county_mapping=df_weather_station_to_county_mapping\n    )\n    \n    # Add time features\n    feat['hour'] = pd.to_datetime(feat['datetime']).dt.hour\n    feat['day_of_week'] = pd.to_datetime(feat['datetime']).dt.dayofweek\n    feat['month'] = pd.to_datetime(feat['datetime']).dt.month\n    feat['is_weekend'] = feat['day_of_week'].isin([5, 6]).astype(int)\n    \n    # Add lag from state\n    feat = add_lag1_from_state(feat)\n    feat['target_lag_24h'] = feat['target_lag_1']\n    feat['target_lag_168h'] = feat['target_lag_1']\n    \n    # Debug once\n    if not printed_debug:\n        print(f\"First batch: {len(test)} rows → {len(feat)} features\\n\")\n        printed_debug = True\n    \n    # Align features to sample_prediction\n    if \"row_id\" in sample_prediction.columns and \"row_id\" in feat.columns:\n        feat_aligned = sample_prediction[[\"row_id\"]].merge(feat, on=\"row_id\", how=\"left\")\n    else:\n        feat_aligned = feat.reindex(sample_prediction.index)\n    \n    # Prepare X_test\n    X_test = feat_aligned.reindex(columns=improved_features, fill_value=0)\n    X_test = X_test.fillna(0)\n    \n    # Make predictions\n    predictions = model.predict(X_test)\n    predictions = np.maximum(predictions, 0)  \n    \n    # Submit\n    sample_prediction['target'] = predictions\n    env.predict(sample_prediction)\n\n\nprint(f\"Processed {counter} iterations\")\nprint(f\"State size: {len(last_y)} unique series tracked\")","metadata":{},"outputs":[],"execution_count":null}]}
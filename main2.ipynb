{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X7ce-iZHYoKe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "#DATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\n",
        "DATA_DIR = \"./\"\n",
        "\n",
        "df_data = pd.read_csv(DATA_DIR + \"train.csv\")\n",
        "df_client = pd.read_csv(DATA_DIR + \"client.csv\")\n",
        "df_historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
        "df_forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
        "df_electricity_prices = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
        "df_gas_prices = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\n",
        "df_weather_station_to_county_mapping = pd.read_csv(DATA_DIR + \"weather_station_to_county_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper functions: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "vlMgKgPXZRu7",
        "outputId": "5dfd47a1-357f-4057-e9cb-62ead0697c09"
      },
      "outputs": [],
      "source": [
        "def upsample_daily_to_hourly(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
        "    # unsample (process) client data. Client data is reported daily, but we need it hourly.\n",
        "    # So we take ever row (1 day) and duplicate (explode) it into 24 rows\n",
        "    df_hourly = df.copy(deep=True) # deep copy to not make changes to dataframe from parameter\n",
        "    df_hourly[date_col] = pd.to_datetime(df_hourly[date_col])\n",
        "\n",
        "    # create a column of lists, where each list contains [00:00, 01:00, ... 23:00] for that day\n",
        "    df_hourly['datetime'] = df_hourly[date_col].apply(lambda x: [x + pd.Timedelta(hours=i) for i in range(24)])\n",
        "    df_hourly = df_hourly.explode('datetime')\n",
        "\n",
        "    # drop the original daily date column as it's no longer needed\n",
        "    df_hourly = df_hourly.drop(columns=[date_col])\n",
        "    return df_hourly\n",
        "  \n",
        "\n",
        "\n",
        "def process_forecast_weather(df_forecast: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans, aggregates, and pivots forecast weather data.\n",
        "\n",
        "    Logic:\n",
        "    1. Map Lat/Lon to County ID.\n",
        "    2. Convert 'Origin Time' (when forecast was made) to 'Target Time' (when weather happens).\n",
        "    3. Group forecasts into 'batches' (Day 1 forecast vs Day 2 forecast).\n",
        "    4. Average the values per county/hour/batch.\n",
        "    5. Pivot so batches become columns (e.g., temperature_1, temperature_2).\n",
        "    \"\"\"\n",
        "    # Safety Copy\n",
        "    df = df_forecast.copy(deep=True)\n",
        "\n",
        "    # 1. Map Coordinates to County\n",
        "    # We use the dictionary passed in from the main function\n",
        "    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n",
        "\n",
        "    # Filter out valid locations only\n",
        "    df = df[df['county'] != -1]\n",
        "\n",
        "    # 2. Calculate Target Time\n",
        "    # Standardize origin time to 02:00:00 (removes minute/second noise)\n",
        "    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'])\n",
        "    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'].dt.date.astype(str) + ' 02:00:00')\n",
        "\n",
        "    # Target Time = Origin + Hours Ahead\n",
        "    df['forecast_datetime'] = df['origin_datetime'] + pd.to_timedelta(df['hours_ahead'], unit='h')\n",
        "\n",
        "    # We don't need origin time anymore\n",
        "    df.drop(columns=['origin_datetime'], inplace=True)\n",
        "\n",
        "    # 3. Create \"Batches\" (cumcount) todo:rename\n",
        "    # A batch represents how far out the forecast is (Day 1 vs Day 2)\n",
        "    # (hours_ahead - 1) // 24 + 1 results in: 1 for 0-24h, 2 for 25-48h\n",
        "    df['cumcount'] = (df['hours_ahead'] - 1) // 24 + 1\n",
        "\n",
        "    # 4. Aggregate (Mean) by County, Time, and Batch\n",
        "    # Identify feature columns (exclude IDs and Time)\n",
        "    exclude_cols = ['latitude', 'longitude', 'hours_ahead', 'forecast_datetime', 'cumcount', 'county', 'data_block_id']\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    agg_dict = {col: 'mean' for col in feature_cols}\n",
        "    # We must keep 'cumcount' in the groupby keys, so we don't aggregate it\n",
        "\n",
        "    df_grouped = df.groupby(['county', 'forecast_datetime', 'cumcount']).agg(agg_dict)\n",
        "\n",
        "    # 5. Pivot (Unstack)\n",
        "    # Moves 'cumcount' from a row index to a column suffix\n",
        "    df_pivoted = df_grouped.unstack(level=-1)\n",
        "\n",
        "    # Flatten MultiIndex columns: ('temperature', 1) -> 'temperature_1'\n",
        "    df_pivoted.columns = [f'{col[0]}_{col[1]}' for col in df_pivoted.columns]\n",
        "\n",
        "    df_pivoted.reset_index(inplace=True)\n",
        "    df_pivoted.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
        "\n",
        "    # Handle missing values (if a forecast is missing, fill with 0 or strictly manage it)\n",
        "    df_pivoted.fillna(0, inplace=True)\n",
        "\n",
        "    return df_pivoted\n",
        "\n",
        "def process_historical_weather(df_historical: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n",
        "    #compose historical weather data (averaging stations per county).\n",
        "    df = df_historical.copy(deep=True)\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n",
        "    df = df[df['county'] != -1]\n",
        "\n",
        "    exclude_cols = ['latitude', 'longitude', 'datetime', 'county', 'data_block_id']\n",
        "    agg_dict = {col: 'mean' for col in df.columns if col not in exclude_cols}\n",
        "    df_grouped = df.groupby(['county', 'datetime']).agg(agg_dict)\n",
        "    df_grouped.reset_index(inplace=True)\n",
        "\n",
        "    return df_grouped\n",
        "\n",
        "def generate_features(\n",
        "    df_data: pd.DataFrame,\n",
        "    df_client: pd.DataFrame,\n",
        "    df_gas_prices: pd.DataFrame,\n",
        "    df_electricity_prices: pd.DataFrame,\n",
        "    df_historical_weather: pd.DataFrame,\n",
        "    df_forecast_weather: pd.DataFrame,\n",
        "    df_weather_station_to_county_mapping: pd.DataFrame,\n",
        "    train_start = '2021-09-01 11:00:00'\n",
        "):\n",
        "  # The weather data uses Latitude/Longitude, but the energy data uses \"Counties\".\n",
        "  # We need a dictionary to translate coordinates into county IDs so we can join them later.\n",
        "  # dictionary: {(lat, lon) -> county_id}\n",
        "  df_weather_station_to_county_mapping = df_weather_station_to_county_mapping[\n",
        "      df_weather_station_to_county_mapping.notnull().all(axis=1)\n",
        "    ].sort_values(by=\"county\")\n",
        "  result_dict = dict(zip(\n",
        "      zip(\n",
        "        round(df_weather_station_to_county_mapping['latitude'],1),\n",
        "        round(df_weather_station_to_county_mapping['longitude'],1)),\n",
        "      df_weather_station_to_county_mapping['county']))\n",
        "  df_historical_weather = df_historical_weather[df_historical_weather['datetime'] >= train_start]\n",
        "\n",
        "  # ---------------------- client data -----------------------\n",
        "  df_client_hourly = upsample_daily_to_hourly(df_client, date_col='date')\n",
        "  if 'data_block_id' in df_client_hourly.columns:\n",
        "    df_client_hourly.drop(columns=['data_block_id'], inplace=True)\n",
        "\n",
        "  # merge into our main dataframe\n",
        "  df_data['datetime'] = pd.to_datetime(df_data['datetime'])\n",
        "  df_data = df_data.merge(df_client_hourly, on=['county','product_type','is_business','datetime'], how='left')\n",
        "  # Filter data to ensure we don't go past the available client data\n",
        "  client_end_date = df_client_hourly['datetime'].max()\n",
        "  df_data = df_data[df_data['datetime'] <= client_end_date]\n",
        "\n",
        "  # --------------- gas prices -------------------\n",
        "  df_gas_hourly = upsample_daily_to_hourly(df_gas_prices, date_col='forecast_date')\n",
        "  cols_to_drop = ['origin_date', 'data_block_id']\n",
        "  df_gas_hourly.drop(columns=[c for c in cols_to_drop if c in df_gas_hourly.columns], inplace=True)\n",
        "\n",
        "  gas_end_date = df_gas_hourly['datetime'].max()\n",
        "  df_data = df_data[df_data['datetime'] <= gas_end_date]\n",
        "\n",
        "  df_data = df_data.merge(df_gas_hourly, on=['datetime'], how='left')\n",
        "\n",
        "  #--------------- electricity (already hourly) ------------------------------\n",
        "  df_electricity_prices_try = df_electricity_prices.copy(deep=True)\n",
        "  if 'origin_date' in df_electricity_prices_try.columns:\n",
        "    df_electricity_prices_try.drop(columns=['origin_date'], inplace=True)\n",
        "  if 'data_block_id' in df_electricity_prices_try.columns:\n",
        "    df_electricity_prices_try.drop(columns=['data_block_id'], inplace=True)\n",
        "  df_electricity_prices_try['forecast_date'] = pd.to_datetime(df_electricity_prices_try['forecast_date'])\n",
        "  df_electricity_prices_try.rename(columns={\"forecast_date\": \"datetime\"}, inplace=True)\n",
        "  df_data = df_data.merge(df_electricity_prices_try, on=['datetime'], how='left')\n",
        "\n",
        "  # --- forecast weather ---\n",
        "  df_forecast_processed = process_forecast_weather(df_forecast_weather, result_dict)\n",
        "  df_data = df_data.merge(df_forecast_processed, on=['county', 'datetime'], how='left')\n",
        "\n",
        "\n",
        "  # --- process historical weather ---\n",
        "  df_weather_processed = process_historical_weather(df_historical_weather, result_dict)\n",
        "  df_data = df_data.merge(df_weather_processed, on=['county', 'datetime'], how='left')\n",
        "\n",
        "  return df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering all data to common end date: 2023-05-29 00:00:00\n",
            "df_data: 2021-09-01 00:00:00 → 2023-05-29 23:00:00\n",
            "Rows: 2,012,112\n"
          ]
        }
      ],
      "source": [
        "# this is to get rid of NaN values\n",
        "min_end_date = min([\n",
        "    pd.to_datetime(df_client['date']).max(),\n",
        "    pd.to_datetime(df_gas_prices['forecast_date']).max(),\n",
        "    pd.to_datetime(df_electricity_prices['forecast_date']).max(),\n",
        "    pd.to_datetime(df_historical_weather['datetime']).max(),\n",
        "    pd.to_datetime(df_forecast_weather['origin_datetime']).max()\n",
        "])\n",
        "\n",
        "print(f\"Filtering all data to common end date: {min_end_date}\")\n",
        "\n",
        "max_datetime = min_end_date + pd.Timedelta(hours=23)\n",
        "df_data = df_data[pd.to_datetime(df_data['datetime']) <= max_datetime].copy()\n",
        "\n",
        "print(f\"df_data: {df_data['datetime'].min()} → {df_data['datetime'].max()}\")\n",
        "print(f\"Rows: {len(df_data):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cz4cU4KG2TiW"
      },
      "outputs": [],
      "source": [
        "combined_df = generate_features(\n",
        "    df_data,\n",
        "    df_client,\n",
        "    df_gas_prices,\n",
        "    df_electricity_prices,\n",
        "    df_historical_weather,\n",
        "    df_forecast_weather,\n",
        "    df_weather_station_to_county_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "AvBjfoKj2zrB",
        "outputId": "c14d17bf-1e45-4118-8429-29b37091bcae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>county</th>\n",
              "      <th>is_business</th>\n",
              "      <th>product_type</th>\n",
              "      <th>target</th>\n",
              "      <th>is_consumption</th>\n",
              "      <th>datetime</th>\n",
              "      <th>data_block_id</th>\n",
              "      <th>row_id</th>\n",
              "      <th>prediction_unit_id</th>\n",
              "      <th>eic_count</th>\n",
              "      <th>...</th>\n",
              "      <th>surface_pressure</th>\n",
              "      <th>cloudcover_total</th>\n",
              "      <th>cloudcover_low</th>\n",
              "      <th>cloudcover_mid</th>\n",
              "      <th>cloudcover_high</th>\n",
              "      <th>windspeed_10m</th>\n",
              "      <th>winddirection_10m</th>\n",
              "      <th>shortwave_radiation</th>\n",
              "      <th>direct_solar_radiation</th>\n",
              "      <th>diffuse_radiation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>96.590</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>17.314</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.904</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-09-01</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>688</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   county  is_business  product_type  target  is_consumption   datetime  \\\n",
              "0       0            0             1   0.713               0 2021-09-01   \n",
              "1       0            0             1  96.590               1 2021-09-01   \n",
              "2       0            0             2   0.000               0 2021-09-01   \n",
              "3       0            0             2  17.314               1 2021-09-01   \n",
              "4       0            0             3   2.904               0 2021-09-01   \n",
              "\n",
              "   data_block_id  row_id  prediction_unit_id  eic_count  ...  \\\n",
              "0              0       0                   0        108  ...   \n",
              "1              0       1                   0        108  ...   \n",
              "2              0       2                   1         17  ...   \n",
              "3              0       3                   1         17  ...   \n",
              "4              0       4                   2        688  ...   \n",
              "\n",
              "   surface_pressure  cloudcover_total  cloudcover_low  cloudcover_mid  \\\n",
              "0               NaN               NaN             NaN             NaN   \n",
              "1               NaN               NaN             NaN             NaN   \n",
              "2               NaN               NaN             NaN             NaN   \n",
              "3               NaN               NaN             NaN             NaN   \n",
              "4               NaN               NaN             NaN             NaN   \n",
              "\n",
              "   cloudcover_high  windspeed_10m  winddirection_10m  shortwave_radiation  \\\n",
              "0              NaN            NaN                NaN                  NaN   \n",
              "1              NaN            NaN                NaN                  NaN   \n",
              "2              NaN            NaN                NaN                  NaN   \n",
              "3              NaN            NaN                NaN                  NaN   \n",
              "4              NaN            NaN                NaN                  NaN   \n",
              "\n",
              "   direct_solar_radiation  diffuse_radiation  \n",
              "0                     NaN                NaN  \n",
              "1                     NaN                NaN  \n",
              "2                     NaN                NaN  \n",
              "3                     NaN                NaN  \n",
              "4                     NaN                NaN  \n",
              "\n",
              "[5 rows x 52 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tuUV-uq29ni",
        "outputId": "c3b1d449-8bc6-41a0-f69e-a37294bda93e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n",
              "       'datetime', 'data_block_id', 'row_id', 'prediction_unit_id',\n",
              "       'eic_count', 'installed_capacity', 'lowest_price_per_mwh',\n",
              "       'highest_price_per_mwh', 'euros_per_mwh', 'temperature_1',\n",
              "       'temperature_2', 'dewpoint_1', 'dewpoint_2', 'cloudcover_high_1',\n",
              "       'cloudcover_high_2', 'cloudcover_low_1', 'cloudcover_low_2',\n",
              "       'cloudcover_mid_1', 'cloudcover_mid_2', 'cloudcover_total_1',\n",
              "       'cloudcover_total_2', '10_metre_u_wind_component_1',\n",
              "       '10_metre_u_wind_component_2', '10_metre_v_wind_component_1',\n",
              "       '10_metre_v_wind_component_2', 'direct_solar_radiation_1',\n",
              "       'direct_solar_radiation_2', 'surface_solar_radiation_downwards_1',\n",
              "       'surface_solar_radiation_downwards_2', 'snowfall_1', 'snowfall_2',\n",
              "       'total_precipitation_1', 'total_precipitation_2', 'temperature',\n",
              "       'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total',\n",
              "       'cloudcover_low', 'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
              "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
              "       'diffuse_radiation'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN ANALYSIS\n",
            "Found NaN values in 40 columns:\n",
            "\n",
            "winddirection_10m: 31,848 (1.58%)\n",
            "shortwave_radiation: 31,848 (1.58%)\n",
            "cloudcover_high: 31,848 (1.58%)\n",
            "windspeed_10m: 31,848 (1.58%)\n",
            "direct_solar_radiation: 31,848 (1.58%)\n",
            "diffuse_radiation: 31,848 (1.58%)\n",
            "cloudcover_mid: 31,848 (1.58%)\n",
            "cloudcover_low: 31,848 (1.58%)\n",
            "rain: 31,848 (1.58%)\n",
            "snowfall: 31,848 (1.58%)\n",
            "cloudcover_total: 31,848 (1.58%)\n",
            "surface_pressure: 31,848 (1.58%)\n",
            "temperature: 31,848 (1.58%)\n",
            "dewpoint: 31,848 (1.58%)\n",
            "temperature_1: 30,888 (1.54%)\n",
            "temperature_2: 30,888 (1.54%)\n",
            "snowfall_2: 30,888 (1.54%)\n",
            "snowfall_1: 30,888 (1.54%)\n",
            "cloudcover_high_2: 30,888 (1.54%)\n",
            "cloudcover_high_1: 30,888 (1.54%)\n",
            "dewpoint_1: 30,888 (1.54%)\n",
            "dewpoint_2: 30,888 (1.54%)\n",
            "cloudcover_low_1: 30,888 (1.54%)\n",
            "cloudcover_low_2: 30,888 (1.54%)\n",
            "10_metre_u_wind_component_2: 30,888 (1.54%)\n",
            "10_metre_u_wind_component_1: 30,888 (1.54%)\n",
            "surface_solar_radiation_downwards_2: 30,888 (1.54%)\n",
            "surface_solar_radiation_downwards_1: 30,888 (1.54%)\n",
            "direct_solar_radiation_2: 30,888 (1.54%)\n",
            "direct_solar_radiation_1: 30,888 (1.54%)\n",
            "10_metre_v_wind_component_2: 30,888 (1.54%)\n",
            "10_metre_v_wind_component_1: 30,888 (1.54%)\n",
            "cloudcover_mid_2: 30,888 (1.54%)\n",
            "cloudcover_mid_1: 30,888 (1.54%)\n",
            "cloudcover_total_2: 30,888 (1.54%)\n",
            "cloudcover_total_1: 30,888 (1.54%)\n",
            "total_precipitation_2: 30,888 (1.54%)\n",
            "total_precipitation_1: 30,888 (1.54%)\n",
            "target: 528 (0.03%)\n",
            "euros_per_mwh: 266 (0.01%)\n",
            "\n",
            "Total NaN: 1,187,978\n",
            "Total cells: 104,629,824\n",
            "NaN percentage: 1.14%\n",
            "Dataset shape: (2012112, 52)\n",
            "Date range: 2021-09-01 00:00:00 → 2023-05-29 23:00:00\n"
          ]
        }
      ],
      "source": [
        "# Analyze NaN values\n",
        "nan_summary = combined_df.isnull().sum() #checking for NaN values in each column, sum counts how many true values (NaN) in each column\n",
        "nan_columns = nan_summary[nan_summary > 0].sort_values(ascending=False) #this filters only columns with NaN values and sorts them in descending order\n",
        "\n",
        "print(\"NaN ANALYSIS\")\n",
        "\n",
        "if len(nan_columns) == 0: #counting the number of columns with NaN values\n",
        "    print(\"NO NaN VALUES FOUND\") \n",
        "else:\n",
        "    print(f\"Found NaN values in {len(nan_columns)} columns:\\n\") \n",
        "    for col, count in nan_columns.items(): #iterating through columns with NaN values\n",
        "        pct = (count / len(combined_df)) * 100 #calculating percentage of NaN values in that column\n",
        "        print(f\"{col}: {count:,} ({pct:.2f}%)\") #printing column name, count of NaN values and percentage\n",
        "    \n",
        "    print(f\"\\nTotal NaN: {combined_df.isnull().sum().sum():,}\") #first sum counts true NaN values in each column, second sum adds them all together\n",
        "    print(f\"Total cells: {combined_df.size:,}\")\n",
        "    print(f\"NaN percentage: {(combined_df.isnull().sum().sum() / combined_df.size) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "print(f\"Dataset shape: {combined_df.shape}\")\n",
        "print(f\"Date range: {combined_df['datetime'].min()} → {combined_df['datetime'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Handling NaN values...\n",
            "Before: 1,187,978 NaN values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\merlink0\\AppData\\Local\\Temp\\ipykernel_15380\\4145414806.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  combined_df['euros_per_mwh'] = combined_df['euros_per_mwh'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After: 528 NaN values\n",
            "Remaining NaN in 'target': 528 (expected for test data)\n"
          ]
        }
      ],
      "source": [
        "# Defining column groups so we could use them later in the loop\n",
        "weather_cols = [\n",
        "    'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', \n",
        "    'cloudcover_mid', 'cloudcover_total', 'windspeed_10m', \n",
        "    'winddirection_10m', 'rain', 'snowfall', 'surface_pressure',\n",
        "    'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation'\n",
        "]\n",
        "\n",
        "# Finding all columns for predictions\n",
        "forecast_cols = [col for col in combined_df.columns if col.endswith('_1') or col.endswith('_2')]\n",
        "\n",
        "\n",
        "print(\"Handling NaN values...\")\n",
        "print(f\"Before: {combined_df.isnull().sum().sum():,} NaN values\")\n",
        "\n",
        "# Sorting by county and time\n",
        "combined_df = combined_df.sort_values(['county', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "# 1. Interpolate weather data per county - using values we already have to calculate missing values by imitating natural change/ linear change\n",
        "# Works well with temperature data\n",
        "for col in weather_cols + forecast_cols:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df[col] = combined_df.groupby('county')[col].transform(\n",
        "            lambda x: x.interpolate(method='linear', limit_direction='both')\n",
        "        )\n",
        "\n",
        "# 2. Fill remaining NaN with forward/backward fill - copying nearest known value until the next known value\n",
        "# Works well with stable values - forward. Works well if data starts later and the first value is most likely the same - backward fill.\n",
        "# Here we use both forward and backward fill to ensure no NaN values remain and we use forward first because we want to prioritize recent known values/ what was the latest known value.\n",
        "combined_df[weather_cols] = combined_df.groupby('county')[weather_cols].ffill().bfill() \n",
        "combined_df[forecast_cols] = combined_df.groupby('county')[forecast_cols].ffill().bfill() \n",
        "\n",
        "# 3. Handling electricity prices\n",
        "combined_df['euros_per_mwh'] = combined_df['euros_per_mwh'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# 4. Final safety: fill any remaining with 0 (very rare)\n",
        "weather_and_forecast = [col for col in weather_cols + forecast_cols if col in combined_df.columns]\n",
        "combined_df[weather_and_forecast] = combined_df[weather_and_forecast].fillna(0)\n",
        "combined_df['euros_per_mwh'] = combined_df['euros_per_mwh'].fillna(combined_df['euros_per_mwh'].mean())\n",
        "\n",
        "print(f\"After: {combined_df.isnull().sum().sum():,} NaN values\")\n",
        "print(f\"Remaining NaN in 'target': {combined_df['target'].isnull().sum()} (expected for test data)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (from xgboost) (2.3.5)\n",
            "Requirement already satisfied: scipy in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (from xgboost) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled train rows: 999,468\n",
            "\n",
            "Training XGBoost...\n",
            "[0]\tvalidation_0-rmse:880.31271\n",
            "[20]\tvalidation_0-rmse:412.01673\n",
            "[40]\tvalidation_0-rmse:268.37377\n",
            "[60]\tvalidation_0-rmse:220.24941\n",
            "[80]\tvalidation_0-rmse:201.45002\n",
            "[100]\tvalidation_0-rmse:191.31822\n",
            "[120]\tvalidation_0-rmse:182.78789\n",
            "[140]\tvalidation_0-rmse:175.70998\n",
            "[160]\tvalidation_0-rmse:170.49251\n",
            "[180]\tvalidation_0-rmse:165.43575\n",
            "[199]\tvalidation_0-rmse:161.18926\n",
            "MODEL PERFORMANCE\n",
            "Train MAE:  67.1620\n",
            "Val MAE:    68.1940\n",
            "Train RMSE: 155.1686\n",
            "Val RMSE:   161.1893\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "               feature  importance\n",
            "18      target_lag_24h    0.435558\n",
            "19     target_lag_168h    0.127719\n",
            "3       is_consumption    0.119138\n",
            "1          is_business    0.046995\n",
            "5   installed_capacity    0.041308\n",
            "15         day_of_week    0.031271\n",
            "12       temperature_1    0.027415\n",
            "0               county    0.023491\n",
            "10    cloudcover_total    0.020867\n",
            "4            eic_count    0.020273\n",
            "\n",
            "Test predictions made: 528\n",
            "Prediction range: -104.38 to 6632.77\n",
            "Mean prediction: 181.24\n",
            "\n",
            "Sample predictions:\n",
            "                  datetime  county  product_type  is_business  \\\n",
            "18768  2021-10-31 03:00:00       0             0            1   \n",
            "18769  2021-10-31 03:00:00       0             0            1   \n",
            "68160  2022-03-27 03:00:00       0             0            1   \n",
            "68161  2022-03-27 03:00:00       0             0            1   \n",
            "141072 2022-10-30 03:00:00       0             0            1   \n",
            "141073 2022-10-30 03:00:00       0             0            1   \n",
            "190464 2023-03-26 03:00:00       0             0            1   \n",
            "190465 2023-03-26 03:00:00       0             0            1   \n",
            "18762  2021-10-31 03:00:00       0             1            0   \n",
            "18763  2021-10-31 03:00:00       0             1            0   \n",
            "\n",
            "        predicted_target  \n",
            "18768          -3.706432  \n",
            "18769         162.611313  \n",
            "68160           0.548360  \n",
            "68161         293.664368  \n",
            "141072          0.748909  \n",
            "141073        355.013367  \n",
            "190464         11.028194  \n",
            "190465        595.085693  \n",
            "18762          12.545994  \n",
            "18763         118.289658  \n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# FEATURE ENGINEERING\n",
        "\n",
        "# Time features\n",
        "combined_df['hour'] = pd.to_datetime(combined_df['datetime']).dt.hour\n",
        "#this is improtant because energy consumption depends on the hour of the day\n",
        "combined_df['day_of_week'] = pd.to_datetime(combined_df['datetime']).dt.dayofweek\n",
        "#this is important because enery consumption varies between weekdays and weekends.\n",
        "combined_df['month'] = pd.to_datetime(combined_df['datetime']).dt.month\n",
        "#important because energy consumption can vary by month (seasonality)\n",
        "combined_df['is_weekend'] = combined_df['day_of_week'].isin([5, 6]).astype(int)\n",
        "# weekends have different consumption patterns\n",
        "\n",
        "# Lag features (historical consumption)\n",
        "combined_df = combined_df.sort_values(['county', 'product_type', 'is_business', 'datetime'])\n",
        "combined_df['target_lag_24h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(24)\n",
        "combined_df['target_lag_168h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(168)\n",
        "#MOST important features - this is the basis of predictions for energy consumption\n",
        "\n",
        "# Features set\n",
        "improved_features = [\n",
        "    'county', 'is_business', 'product_type', 'is_consumption',\n",
        "    'eic_count', 'installed_capacity',\n",
        "    'euros_per_mwh', 'lowest_price_per_mwh', 'highest_price_per_mwh',\n",
        "    'temperature', 'cloudcover_total', 'rain',\n",
        "    'temperature_1', 'temperature_2',\n",
        "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
        "    'target_lag_24h', 'target_lag_168h',\n",
        "]\n",
        "\n",
        "# Split\n",
        "train_df = combined_df[combined_df['target'].notna()].copy()\n",
        "test_df = combined_df[combined_df['target'].isna()].copy()\n",
        "\n",
        "# Removing rows with NaN in lag features (first 168 hours). This makes up abou 1.5% of the data. \n",
        "# This contain only rows where we don't have enough history to create lag features - 168 hours × county/product/business kombinatsioonid\n",
        "train_df = train_df.dropna(subset=['target_lag_24h', 'target_lag_168h'])\n",
        "\n",
        "# Sample 50% \n",
        "train_sample = train_df.sample(frac=0.5, random_state=42)\n",
        "print(f\"Sampled train rows: {len(train_sample):,}\")\n",
        "\n",
        "X = train_sample[improved_features].copy()\n",
        "y = train_sample['target'].copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Model\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 3,\n",
        "    'gamma': 0.1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'early_stopping_rounds': 20,\n",
        "}\n",
        "\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "model = xgb.XGBRegressor(**params)\n",
        "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=20)\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "# Metrics\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(f\"Train MAE:  {train_mae:.4f}\")\n",
        "print(f\"Val MAE:    {val_mae:.4f}\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Val RMSE:   {val_rmse:.4f}\")\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "import pandas as pd\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': improved_features, \n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "\n",
        "# 4. PREDICTIONS ON TEST DATA\n",
        "\n",
        "# Preparing test features\n",
        "X_test = test_df[improved_features].copy()\n",
        "\n",
        "# Checking for NaN in test features\n",
        "test_nan_count = X_test.isnull().sum().sum()\n",
        "if test_nan_count > 0:\n",
        "    print(f\" {test_nan_count} NaN values in test features\")\n",
        "    print(\"Filling NaN with 0...\")\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "# Making predictions\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Adding predictions to test dataframe\n",
        "test_df['predicted_target'] = test_predictions\n",
        "\n",
        "print(f\"\\nTest predictions made: {len(test_predictions)}\")\n",
        "print(f\"Prediction range: {test_predictions.min():.2f} to {test_predictions.max():.2f}\")\n",
        "print(f\"Mean prediction: {test_predictions.mean():.2f}\")\n",
        "\n",
        "# Sample predictions\n",
        "print(\"\\nSample predictions:\")\n",
        "print(test_df[['datetime', 'county', 'product_type', 'is_business', 'predicted_target']].head(10))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled train rows: 1,398,516\n",
            "[0]\tvalidation_0-rmse:902.91934\n",
            "[20]\tvalidation_0-rmse:524.58631\n",
            "[40]\tvalidation_0-rmse:326.07719\n",
            "[60]\tvalidation_0-rmse:230.20701\n",
            "[80]\tvalidation_0-rmse:186.34827\n",
            "[100]\tvalidation_0-rmse:165.98828\n",
            "[120]\tvalidation_0-rmse:155.88461\n",
            "[140]\tvalidation_0-rmse:149.58479\n",
            "[160]\tvalidation_0-rmse:145.30097\n",
            "[180]\tvalidation_0-rmse:141.79791\n",
            "[200]\tvalidation_0-rmse:139.60287\n",
            "[220]\tvalidation_0-rmse:137.65919\n",
            "[240]\tvalidation_0-rmse:135.98184\n",
            "[260]\tvalidation_0-rmse:134.36455\n",
            "[280]\tvalidation_0-rmse:132.67342\n",
            "[299]\tvalidation_0-rmse:131.23348\n",
            "Model V2 performance\n",
            "Train MAE:  42.1592\n",
            "Val MAE:    43.9662\n",
            "Train RMSE: 118.0630\n",
            "Val RMSE:   131.2335\n",
            "\n",
            "Top 15 Most Important Features:\n",
            "                     feature  importance\n",
            "19            target_lag_48h    0.494996\n",
            "18            target_lag_24h    0.207594\n",
            "20            target_lag_72h    0.067318\n",
            "26    capacity_x_consumption    0.056321\n",
            "3             is_consumption    0.044529\n",
            "22   target_rolling_24h_mean    0.016621\n",
            "17                is_weekend    0.010381\n",
            "24               temp_x_hour    0.009878\n",
            "5         installed_capacity    0.009430\n",
            "21           target_lag_168h    0.009354\n",
            "15               day_of_week    0.008958\n",
            "23  target_rolling_168h_mean    0.008908\n",
            "1                is_business    0.007414\n",
            "14                      hour    0.006679\n",
            "13             temperature_2    0.005812\n",
            "Test predictions (non-negative)\n",
            "Predictions made: 528\n",
            "Prediction range: 0.00 to 6929.65\n",
            "Mean prediction: 177.48\n",
            "Negative predictions fixed: 86\n",
            "\n",
            "Sample predictions:\n",
            "                  datetime  county  product_type  predicted_target\n",
            "18768  2021-10-31 03:00:00       0             0          2.189416\n",
            "18769  2021-10-31 03:00:00       0             0        116.148499\n",
            "68160  2022-03-27 03:00:00       0             0          6.607377\n",
            "68161  2022-03-27 03:00:00       0             0        245.327667\n",
            "141072 2022-10-30 03:00:00       0             0          0.068535\n",
            "141073 2022-10-30 03:00:00       0             0        269.789978\n",
            "190464 2023-03-26 03:00:00       0             0          0.000000\n",
            "190465 2023-03-26 03:00:00       0             0        575.898987\n",
            "18762  2021-10-31 03:00:00       0             1          0.000000\n",
            "18763  2021-10-31 03:00:00       0             1        129.100784\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Additional improvements, trying if we can get better performance\n",
        "\n",
        "# 1. Ading more LAG features since they are the most imprtant\n",
        "combined_df = combined_df.sort_values(['county', 'product_type', 'is_business', 'datetime'])\n",
        "\n",
        "# Rolling averages for smoother trends\n",
        "combined_df['target_rolling_24h_mean'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=24, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "combined_df['target_rolling_168h_mean'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=168, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Lag 48h and 72h\n",
        "combined_df['target_lag_48h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(48)\n",
        "combined_df['target_lag_72h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(72)\n",
        "\n",
        "# 2. Interaction features \n",
        "combined_df['temp_x_hour'] = combined_df['temperature'] * combined_df['hour']\n",
        "combined_df['price_x_hour'] = combined_df['euros_per_mwh'] * combined_df['hour']\n",
        "combined_df['capacity_x_consumption'] = combined_df['installed_capacity'] * combined_df['is_consumption']\n",
        "\n",
        "# 3. Improved feature set\n",
        "improved_features_v2 = [\n",
        "    'county', 'is_business', 'product_type', 'is_consumption',\n",
        "    'eic_count', 'installed_capacity',\n",
        "    'euros_per_mwh', 'lowest_price_per_mwh', 'highest_price_per_mwh',\n",
        "    'temperature', 'cloudcover_total', 'rain',\n",
        "    'temperature_1', 'temperature_2',\n",
        "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
        "    'target_lag_24h', 'target_lag_48h', 'target_lag_72h', 'target_lag_168h',\n",
        "    'target_rolling_24h_mean', 'target_rolling_168h_mean',\n",
        "    'temp_x_hour', 'price_x_hour', 'capacity_x_consumption',\n",
        "]\n",
        "\n",
        "# Split\n",
        "train_df = combined_df[combined_df['target'].notna()].copy()\n",
        "test_df = combined_df[combined_df['target'].isna()].copy()\n",
        "\n",
        "# Removing NaN rows\n",
        "train_df = train_df.dropna(subset=improved_features_v2)\n",
        "\n",
        "# Using 70% data now (cause more is better with more features)\n",
        "train_sample = train_df.sample(frac=0.7, random_state=42)\n",
        "print(f\"Sampled train rows: {len(train_sample):,}\")\n",
        "\n",
        "X = train_sample[improved_features_v2].copy()\n",
        "y = train_sample['target'].copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Improved model v2\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 7,  # 6 → 7\n",
        "    'learning_rate': 0.03,  # 0.05 → 0.03 (slower learning rate for better convergence)\n",
        "    'n_estimators': 300,  # 200 → 300\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 5,  # 3 → 5\n",
        "    'gamma': 0.2,  # 0.1 → 0.2\n",
        "    'reg_alpha': 0.1,  # L1 regularization\n",
        "    'reg_lambda': 1.0,  # L2 regularization\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'early_stopping_rounds': 30,  # 20 → 30\n",
        "}\n",
        "\n",
        "model = xgb.XGBRegressor(**params)\n",
        "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=20)\n",
        "\n",
        "#Evaluation\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "print(\"Model V2 performance\")\n",
        "print(f\"Train MAE:  {train_mae:.4f}\")\n",
        "print(f\"Val MAE:    {val_mae:.4f}\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Val RMSE:   {val_rmse:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': improved_features_v2,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(feature_importance.head(15))\n",
        "for idx, row in feature_importance.head(15).iterrows():\n",
        "    print(f\"  {row['feature']:30s} : {row['importance']:.4f} ({row['importance']*100:.2f}%)\")\n",
        "\n",
        "\n",
        "# Predictions with non-negative constraint\n",
        "X_test = test_df[improved_features_v2].copy()\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Fixing negative predictions\n",
        "test_predictions = np.maximum(test_predictions, 0) \n",
        "\n",
        "test_df['predicted_target'] = test_predictions\n",
        "\n",
        "print(\"Test predictions (non-negative)\")\n",
        "print(f\"Predictions made: {len(test_predictions)}\")\n",
        "print(f\"Prediction range: {test_predictions.min():.2f} to {test_predictions.max():.2f}\")\n",
        "print(f\"Mean prediction: {test_predictions.mean():.2f}\")\n",
        "print(f\"Negative predictions fixed: {(model.predict(X_test) < 0).sum()}\")\n",
        "\n",
        "# Sample\n",
        "print(\"\\nSample predictions:\")\n",
        "print(test_df[['datetime', 'county', 'product_type', 'predicted_target']].head(10))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

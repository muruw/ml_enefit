{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X7ce-iZHYoKe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "#DATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\n",
        "DATA_DIR = \"./\"\n",
        "\n",
        "df_data = pd.read_csv(DATA_DIR + \"train.csv\")\n",
        "df_client = pd.read_csv(DATA_DIR + \"client.csv\")\n",
        "df_historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
        "df_forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
        "df_electricity_prices = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
        "df_gas_prices = pd.read_csv(DATA_DIR + \"gas_prices.csv\")\n",
        "df_weather_station_to_county_mapping = pd.read_csv(DATA_DIR + \"weather_station_to_county_mapping.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFB9zs_Nn1Hv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "vlMgKgPXZRu7",
        "outputId": "5dfd47a1-357f-4057-e9cb-62ead0697c09"
      },
      "outputs": [],
      "source": [
        "def upsample_daily_to_hourly(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
        "    # unsample (process) client data. Client data is reported daily, but we need it hourly.\n",
        "    # So we take ever row (1 day) and duplicate (explode) it into 24 rows\n",
        "    df_hourly = df.copy(deep=True) # deep copy to not make changes to dataframe from parameter\n",
        "    df_hourly[date_col] = pd.to_datetime(df_hourly[date_col])\n",
        "\n",
        "    # create a column of lists, where each list contains [00:00, 01:00, ... 23:00] for that day\n",
        "    df_hourly['datetime'] = df_hourly[date_col].apply(lambda x: [x + pd.Timedelta(hours=i) for i in range(24)])\n",
        "    df_hourly = df_hourly.explode('datetime')\n",
        "\n",
        "    # drop the original daily date column as it's no longer needed\n",
        "    df_hourly = df_hourly.drop(columns=[date_col])\n",
        "    return df_hourly\n",
        "\n",
        "\n",
        "# nb! needs more testing as this method was generated by AI.\n",
        "def process_forecast_weather(df_forecast: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans, aggregates, and pivots forecast weather data.\n",
        "\n",
        "    Logic:\n",
        "    1. Map Lat/Lon to County ID.\n",
        "    2. Convert 'Origin Time' (when forecast was made) to 'Target Time' (when weather happens).\n",
        "    3. Group forecasts into 'batches' (Day 1 forecast vs Day 2 forecast).\n",
        "    4. Average the values per county/hour/batch.\n",
        "    5. Pivot so batches become columns (e.g., temperature_1, temperature_2).\n",
        "    \"\"\"\n",
        "    # Safety Copy\n",
        "    df = df_forecast.copy(deep=True)\n",
        "\n",
        "    # 1. Map Coordinates to County\n",
        "    # We use the dictionary passed in from the main function\n",
        "    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n",
        "\n",
        "    # Filter out valid locations only\n",
        "    df = df[df['county'] != -1]\n",
        "\n",
        "    # 2. Calculate Target Time\n",
        "    # Standardize origin time to 02:00:00 (removes minute/second noise)\n",
        "    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'])\n",
        "    df['origin_datetime'] = pd.to_datetime(df['origin_datetime'].dt.date.astype(str) + ' 02:00:00')\n",
        "\n",
        "    # Target Time = Origin + Hours Ahead\n",
        "    df['forecast_datetime'] = df['origin_datetime'] + pd.to_timedelta(df['hours_ahead'], unit='h')\n",
        "\n",
        "    # We don't need origin time anymore\n",
        "    df.drop(columns=['origin_datetime'], inplace=True)\n",
        "\n",
        "    # 3. Create \"Batches\" (cumcount) todo:rename\n",
        "    # A batch represents how far out the forecast is (Day 1 vs Day 2)\n",
        "    # (hours_ahead - 1) // 24 + 1 results in: 1 for 0-24h, 2 for 25-48h\n",
        "    df['cumcount'] = (df['hours_ahead'] - 1) // 24 + 1\n",
        "\n",
        "    # 4. Aggregate (Mean) by County, Time, and Batch\n",
        "    # Identify feature columns (exclude IDs and Time)\n",
        "    exclude_cols = ['latitude', 'longitude', 'hours_ahead', 'forecast_datetime', 'cumcount', 'county', 'data_block_id']\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    agg_dict = {col: 'mean' for col in feature_cols}\n",
        "    # We must keep 'cumcount' in the groupby keys, so we don't aggregate it\n",
        "\n",
        "    df_grouped = df.groupby(['county', 'forecast_datetime', 'cumcount']).agg(agg_dict)\n",
        "\n",
        "    # 5. Pivot (Unstack)\n",
        "    # Moves 'cumcount' from a row index to a column suffix\n",
        "    df_pivoted = df_grouped.unstack(level=-1)\n",
        "\n",
        "    # Flatten MultiIndex columns: ('temperature', 1) -> 'temperature_1'\n",
        "    df_pivoted.columns = [f'{col[0]}_{col[1]}' for col in df_pivoted.columns]\n",
        "\n",
        "    df_pivoted.reset_index(inplace=True)\n",
        "    df_pivoted.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
        "\n",
        "    # Handle missing values (if a forecast is missing, fill with 0 or strictly manage it)\n",
        "    df_pivoted.fillna(0, inplace=True)\n",
        "\n",
        "    return df_pivoted\n",
        "\n",
        "def process_historical_weather(df_historical: pd.DataFrame, location_map: dict) -> pd.DataFrame:\n",
        "    #compose historical weather data (averaging stations per county).\n",
        "    df = df_historical.copy(deep=True)\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    df['county'] = [location_map.get((x, y), -1) for x, y in zip(df['latitude'], df['longitude'])]\n",
        "    df = df[df['county'] != -1]\n",
        "\n",
        "    exclude_cols = ['latitude', 'longitude', 'datetime', 'county', 'data_block_id']\n",
        "    agg_dict = {col: 'mean' for col in df.columns if col not in exclude_cols}\n",
        "    df_grouped = df.groupby(['county', 'datetime']).agg(agg_dict)\n",
        "    df_grouped.reset_index(inplace=True)\n",
        "\n",
        "    return df_grouped\n",
        "\n",
        "def generate_features(\n",
        "    df_data: pd.DataFrame,\n",
        "    df_client: pd.DataFrame,\n",
        "    df_gas_prices: pd.DataFrame,\n",
        "    df_electricity_prices: pd.DataFrame,\n",
        "    df_historical_weather: pd.DataFrame,\n",
        "    df_forecast_weather: pd.DataFrame,\n",
        "    df_weather_station_to_county_mapping: pd.DataFrame,\n",
        "    train_start = '2021-09-01 11:00:00'\n",
        "):\n",
        "  # The weather data uses Latitude/Longitude, but the energy data uses \"Counties\".\n",
        "  # We need a dictionary to translate coordinates into county IDs so we can join them later.\n",
        "  # dictionary: {(lat, lon) -> county_id}\n",
        "  df_weather_station_to_county_mapping = df_weather_station_to_county_mapping[\n",
        "      df_weather_station_to_county_mapping.notnull().all(axis=1)\n",
        "    ].sort_values(by=\"county\")\n",
        "  result_dict = dict(zip(\n",
        "      zip(\n",
        "        round(df_weather_station_to_county_mapping['latitude'],1),\n",
        "        round(df_weather_station_to_county_mapping['longitude'],1)),\n",
        "      df_weather_station_to_county_mapping['county']))\n",
        "  df_historical_weather = df_historical_weather[df_historical_weather['datetime'] >= train_start]\n",
        "\n",
        "  # ---------------------- client data -----------------------\n",
        "  df_client_hourly = upsample_daily_to_hourly(df_client, date_col='date')\n",
        "  if 'data_block_id' in df_client_hourly.columns:\n",
        "    df_client_hourly.drop(columns=['data_block_id'], inplace=True)\n",
        "\n",
        "  # merge into our main dataframe\n",
        "  df_data['datetime'] = pd.to_datetime(df_data['datetime'])\n",
        "  df_data = df_data.merge(df_client_hourly, on=['county','product_type','is_business','datetime'], how='left')\n",
        "  # Filter data to ensure we don't go past the available client data\n",
        "  client_end_date = df_client_hourly['datetime'].max()\n",
        "  df_data = df_data[df_data['datetime'] <= client_end_date]\n",
        "\n",
        "  # --------------- gas prices -------------------\n",
        "  df_gas_hourly = upsample_daily_to_hourly(df_gas_prices, date_col='forecast_date')\n",
        "  cols_to_drop = ['origin_date', 'data_block_id']\n",
        "  df_gas_hourly.drop(columns=[c for c in cols_to_drop if c in df_gas_hourly.columns], inplace=True)\n",
        "\n",
        "  gas_end_date = df_gas_hourly['datetime'].max()\n",
        "  df_data = df_data[df_data['datetime'] <= gas_end_date]\n",
        "\n",
        "  df_data = df_data.merge(df_gas_hourly, on=['datetime'], how='left')\n",
        "\n",
        "  #--------------- electricity (already hourly) ------------------------------\n",
        "  df_electricity_prices_try = df_electricity_prices.copy(deep=True)\n",
        "  if 'origin_date' in df_electricity_prices_try.columns:\n",
        "    df_electricity_prices_try.drop(columns=['origin_date'], inplace=True)\n",
        "  if 'data_block_id' in df_electricity_prices_try.columns:\n",
        "    df_electricity_prices_try.drop(columns=['data_block_id'], inplace=True)\n",
        "  df_electricity_prices_try['forecast_date'] = pd.to_datetime(df_electricity_prices_try['forecast_date'])\n",
        "  df_electricity_prices_try.rename(columns={\"forecast_date\": \"datetime\"}, inplace=True)\n",
        "  df_data = df_data.merge(df_electricity_prices_try, on=['datetime'], how='left')\n",
        "\n",
        "  # --- forecast weather ---\n",
        "  df_forecast_processed = process_forecast_weather(df_forecast_weather, result_dict)\n",
        "  df_data = df_data.merge(df_forecast_processed, on=['county', 'datetime'], how='left')\n",
        "\n",
        "\n",
        "  # --- process historical weather ---\n",
        "  df_weather_processed = process_historical_weather(df_historical_weather, result_dict)\n",
        "  df_data = df_data.merge(df_weather_processed, on=['county', 'datetime'], how='left')\n",
        "\n",
        "  return df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering all data to common end date: 2023-05-29 00:00:00\n",
            "df_data: 2021-09-01 00:00:00 → 2023-05-29 23:00:00\n",
            "Rows: 2,012,112\n"
          ]
        }
      ],
      "source": [
        "# this is to get rid of NaN values\n",
        "min_end_date = min([\n",
        "    pd.to_datetime(df_client['date']).max(),\n",
        "    pd.to_datetime(df_gas_prices['forecast_date']).max(),\n",
        "    pd.to_datetime(df_electricity_prices['forecast_date']).max(),\n",
        "    pd.to_datetime(df_historical_weather['datetime']).max(),\n",
        "    pd.to_datetime(df_forecast_weather['origin_datetime']).max()\n",
        "])\n",
        "\n",
        "print(f\"Filtering all data to common end date: {min_end_date}\")\n",
        "\n",
        "max_datetime = min_end_date + pd.Timedelta(hours=23)\n",
        "df_data = df_data[pd.to_datetime(df_data['datetime']) <= max_datetime].copy()\n",
        "\n",
        "print(f\"df_data: {df_data['datetime'].min()} → {df_data['datetime'].max()}\")\n",
        "print(f\"Rows: {len(df_data):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cz4cU4KG2TiW"
      },
      "outputs": [],
      "source": [
        "combined_df = generate_features(\n",
        "    df_data,\n",
        "    df_client,\n",
        "    df_gas_prices,\n",
        "    df_electricity_prices,\n",
        "    df_historical_weather,\n",
        "    df_forecast_weather,\n",
        "    df_weather_station_to_county_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "AvBjfoKj2zrB",
        "outputId": "c14d17bf-1e45-4118-8429-29b37091bcae"
      },
      "outputs": [],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tuUV-uq29ni",
        "outputId": "c3b1d449-8bc6-41a0-f69e-a37294bda93e"
      },
      "outputs": [],
      "source": [
        "combined_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "NaN ANALYSIS\n",
            "======================================================================\n",
            "Found NaN values in 40 columns:\n",
            "\n",
            "   winddirection_10m: 31,848 (1.58%)\n",
            "   shortwave_radiation: 31,848 (1.58%)\n",
            "   cloudcover_high: 31,848 (1.58%)\n",
            "   windspeed_10m: 31,848 (1.58%)\n",
            "   direct_solar_radiation: 31,848 (1.58%)\n",
            "   diffuse_radiation: 31,848 (1.58%)\n",
            "   cloudcover_mid: 31,848 (1.58%)\n",
            "   cloudcover_low: 31,848 (1.58%)\n",
            "   rain: 31,848 (1.58%)\n",
            "   snowfall: 31,848 (1.58%)\n",
            "   cloudcover_total: 31,848 (1.58%)\n",
            "   surface_pressure: 31,848 (1.58%)\n",
            "   temperature: 31,848 (1.58%)\n",
            "   dewpoint: 31,848 (1.58%)\n",
            "   temperature_1: 30,888 (1.54%)\n",
            "   temperature_2: 30,888 (1.54%)\n",
            "   snowfall_2: 30,888 (1.54%)\n",
            "   snowfall_1: 30,888 (1.54%)\n",
            "   cloudcover_high_2: 30,888 (1.54%)\n",
            "   cloudcover_high_1: 30,888 (1.54%)\n",
            "   dewpoint_1: 30,888 (1.54%)\n",
            "   dewpoint_2: 30,888 (1.54%)\n",
            "   cloudcover_low_1: 30,888 (1.54%)\n",
            "   cloudcover_low_2: 30,888 (1.54%)\n",
            "   10_metre_u_wind_component_2: 30,888 (1.54%)\n",
            "   10_metre_u_wind_component_1: 30,888 (1.54%)\n",
            "   surface_solar_radiation_downwards_2: 30,888 (1.54%)\n",
            "   surface_solar_radiation_downwards_1: 30,888 (1.54%)\n",
            "   direct_solar_radiation_2: 30,888 (1.54%)\n",
            "   direct_solar_radiation_1: 30,888 (1.54%)\n",
            "   10_metre_v_wind_component_2: 30,888 (1.54%)\n",
            "   10_metre_v_wind_component_1: 30,888 (1.54%)\n",
            "   cloudcover_mid_2: 30,888 (1.54%)\n",
            "   cloudcover_mid_1: 30,888 (1.54%)\n",
            "   cloudcover_total_2: 30,888 (1.54%)\n",
            "   cloudcover_total_1: 30,888 (1.54%)\n",
            "   total_precipitation_2: 30,888 (1.54%)\n",
            "   total_precipitation_1: 30,888 (1.54%)\n",
            "   target: 528 (0.03%)\n",
            "   euros_per_mwh: 266 (0.01%)\n",
            "\n",
            "Total NaN: 1,187,978\n",
            "Total cells: 104,629,824\n",
            "NaN percentage: 1.14%\n",
            "Dataset shape: (2012112, 52)\n",
            "Date range: 2021-09-01 00:00:00 → 2023-05-29 23:00:00\n"
          ]
        }
      ],
      "source": [
        "# Analyze NaN values\n",
        "nan_summary = combined_df.isnull().sum()\n",
        "nan_columns = nan_summary[nan_summary > 0].sort_values(ascending=False)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NaN ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(nan_columns) == 0:\n",
        "    print(\"NO NaN VALUES FOUND!\")\n",
        "else:\n",
        "    print(f\"Found NaN values in {len(nan_columns)} columns:\\n\")\n",
        "    for col, count in nan_columns.items():\n",
        "        pct = (count / len(combined_df)) * 100\n",
        "        print(f\"   {col}: {count:,} ({pct:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\nTotal NaN: {combined_df.isnull().sum().sum():,}\")\n",
        "    print(f\"Total cells: {combined_df.size:,}\")\n",
        "    print(f\"NaN percentage: {(combined_df.isnull().sum().sum() / combined_df.size) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "print(f\"Dataset shape: {combined_df.shape}\")\n",
        "print(f\"Date range: {combined_df['datetime'].min()} → {combined_df['datetime'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Handling NaN values...\n",
            "Before: 794 NaN values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\merlink0\\AppData\\Local\\Temp\\ipykernel_24552\\2776406172.py:29: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  combined_df['euros_per_mwh'] = combined_df['euros_per_mwh'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After: 528 NaN values\n",
            "Remaining NaN in 'target': 528 (expected for test data)\n"
          ]
        }
      ],
      "source": [
        "# Defining column groups\n",
        "weather_cols = [\n",
        "    'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', \n",
        "    'cloudcover_mid', 'cloudcover_total', 'windspeed_10m', \n",
        "    'winddirection_10m', 'rain', 'snowfall', 'surface_pressure',\n",
        "    'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation'\n",
        "]\n",
        "\n",
        "forecast_cols = [col for col in combined_df.columns if col.endswith('_1') or col.endswith('_2')]\n",
        "\n",
        "print(\"Handling NaN values...\")\n",
        "print(f\"Before: {combined_df.isnull().sum().sum():,} NaN values\")\n",
        "\n",
        "# Sorting by county and time\n",
        "combined_df = combined_df.sort_values(['county', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "# 1. Interpolate weather data per county\n",
        "for col in weather_cols + forecast_cols:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df[col] = combined_df.groupby('county')[col].transform(\n",
        "            lambda x: x.interpolate(method='linear', limit_direction='both')\n",
        "        )\n",
        "\n",
        "# 2. Fill remaining NaN with forward/backward fill\n",
        "combined_df[weather_cols] = combined_df.groupby('county')[weather_cols].ffill().bfill()\n",
        "combined_df[forecast_cols] = combined_df.groupby('county')[forecast_cols].ffill().bfill()\n",
        "\n",
        "# 3. Handle electricity prices\n",
        "combined_df['euros_per_mwh'] = combined_df['euros_per_mwh'].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# 4. Final safety: fill any remaining with 0 (very rare)\n",
        "weather_and_forecast = [col for col in weather_cols + forecast_cols if col in combined_df.columns]\n",
        "combined_df[weather_and_forecast] = combined_df[weather_and_forecast].fillna(0)\n",
        "combined_df['euros_per_mwh'] = combined_df['euros_per_mwh'].fillna(combined_df['euros_per_mwh'].mean())\n",
        "\n",
        "print(f\"After: {combined_df.isnull().sum().sum():,} NaN values\")\n",
        "print(f\"Remaining NaN in 'target': {combined_df['target'].isnull().sum()} (expected for test data)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.8.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\merlink0\\enefit\\.venv\\lib\\site-packages (from xgboost) (2.3.5)\n",
            "Collecting scipy (from xgboost)\n",
            "  Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Collecting joblib>=1.3.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
            "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
            "   - -------------------------------------- 2.1/72.0 MB 11.7 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 4.7/72.0 MB 11.6 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 7.1/72.0 MB 11.6 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 9.2/72.0 MB 11.0 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 11.5/72.0 MB 11.1 MB/s eta 0:00:06\n",
            "   ------- -------------------------------- 14.2/72.0 MB 11.1 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 16.5/72.0 MB 11.1 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 18.9/72.0 MB 11.2 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 21.5/72.0 MB 11.2 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 23.9/72.0 MB 11.3 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 26.2/72.0 MB 11.3 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 28.6/72.0 MB 11.3 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 31.2/72.0 MB 11.3 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 33.0/72.0 MB 11.2 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 35.1/72.0 MB 11.1 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 37.0/72.0 MB 10.9 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 38.8/72.0 MB 10.7 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 41.2/72.0 MB 10.8 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 43.5/72.0 MB 10.8 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 46.1/72.0 MB 10.8 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 48.2/72.0 MB 10.8 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 50.3/72.0 MB 10.8 MB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 53.2/72.0 MB 10.9 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 55.6/72.0 MB 10.9 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 57.9/72.0 MB 10.9 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 60.3/72.0 MB 10.9 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 62.7/72.0 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 65.3/72.0 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 67.6/72.0 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 70.0/72.0 MB 10.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  71.8/72.0 MB 10.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  71.8/72.0 MB 10.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 72.0/72.0 MB 10.5 MB/s  0:00:06\n",
            "Downloading scikit_learn-1.8.0-cp313-cp313-win_amd64.whl (8.0 MB)\n",
            "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
            "   ----------- ---------------------------- 2.4/8.0 MB 12.1 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.0/8.0 MB 12.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.3/8.0 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.0/8.0 MB 11.1 MB/s  0:00:00\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
            "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 2.4/38.5 MB 11.9 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 5.0/38.5 MB 11.7 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 7.6/38.5 MB 11.8 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 10.0/38.5 MB 11.6 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 12.3/38.5 MB 11.5 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 14.7/38.5 MB 11.5 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 17.6/38.5 MB 11.6 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 19.9/38.5 MB 11.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 22.5/38.5 MB 11.6 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 25.2/38.5 MB 11.6 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 27.5/38.5 MB 11.6 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 30.1/38.5 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 32.8/38.5 MB 11.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 35.1/38.5 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  37.7/38.5 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.3/38.5 MB 11.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 38.5/38.5 MB 11.1 MB/s  0:00:03\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, xgboost, scikit-learn\n",
            "\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   -------- ------------------------------- 1/5 [scipy]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   ------------------------ --------------- 3/5 [xgboost]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   -------------------------------- ------- 4/5 [scikit-learn]\n",
            "   ---------------------------------------- 5/5 [scikit-learn]\n",
            "\n",
            "Successfully installed joblib-1.5.2 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0 xgboost-3.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train rows: 2,011,584\n",
            "Test rows: 528\n",
            "Sampled train rows: 603,475 (30% of full data)\n",
            "\n",
            "X_train shape: (482780, 14)\n",
            "X_val shape: (120695, 14)\n",
            "\n",
            "Training XGBoost...\n",
            "[0]\tvalidation_0-rmse:820.59801\n",
            "[10]\tvalidation_0-rmse:446.21080\n",
            "[20]\tvalidation_0-rmse:344.18446\n",
            "[30]\tvalidation_0-rmse:317.51043\n",
            "[40]\tvalidation_0-rmse:302.94427\n",
            "[50]\tvalidation_0-rmse:296.57800\n",
            "[60]\tvalidation_0-rmse:292.05050\n",
            "[70]\tvalidation_0-rmse:287.56444\n",
            "[80]\tvalidation_0-rmse:285.27242\n",
            "[90]\tvalidation_0-rmse:283.33125\n",
            "[99]\tvalidation_0-rmse:281.38850\n",
            "\n",
            "======================================================================\n",
            "MODEL PERFORMANCE\n",
            "======================================================================\n",
            "Train MAE:  108.7295\n",
            "Val MAE:    110.8259\n",
            "Train RMSE: 270.5967\n",
            "Val RMSE:   281.3885\n",
            "======================================================================\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "               feature  importance\n",
            "1          is_business    0.360651\n",
            "3       is_consumption    0.216104\n",
            "5   installed_capacity    0.168930\n",
            "4            eic_count    0.102530\n",
            "0               county    0.034795\n",
            "9          temperature    0.028011\n",
            "12       temperature_1    0.019687\n",
            "6        euros_per_mwh    0.019338\n",
            "10    cloudcover_total    0.015108\n",
            "13       temperature_2    0.012221\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA PREPARATION - MINIMAL FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "# Define minimal feature set (only most important)\n",
        "minimal_features = [\n",
        "    'county', 'is_business', 'product_type', 'is_consumption',\n",
        "    'eic_count', 'installed_capacity',\n",
        "    'euros_per_mwh',  # Electricity price\n",
        "    'lowest_price_per_mwh', 'highest_price_per_mwh',  # Gas prices\n",
        "    'temperature', 'cloudcover_total', 'rain',  # Basic weather\n",
        "    'temperature_1', 'temperature_2',  # Forecast temps\n",
        "]\n",
        "\n",
        "# Split train/test\n",
        "train_df = combined_df[combined_df['target'].notna()].copy()\n",
        "test_df = combined_df[combined_df['target'].isna()].copy()\n",
        "\n",
        "print(f\"Train rows: {len(train_df):,}\")\n",
        "print(f\"Test rows: {len(test_df):,}\")\n",
        "\n",
        "# Sample train data to save memory (use 30% of data)\n",
        "train_sample = train_df.sample(frac=0.3, random_state=42)\n",
        "print(f\"Sampled train rows: {len(train_sample):,} (30% of full data)\")\n",
        "\n",
        "# Prepare features\n",
        "X = train_sample[minimal_features].copy()\n",
        "y = train_sample['target'].copy()\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nX_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. XGBOOST MODEL - MEMORY EFFICIENT\n",
        "# ============================================================================\n",
        "\n",
        "# Minimal parameters for low memory usage\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'early_stopping_rounds': 10,  # ← Siia sisse!\n",
        "}\n",
        "\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "model = xgb.XGBRegressor(**params)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "# Metrics\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Train MAE:  {train_mae:.4f}\")\n",
        "print(f\"Val MAE:    {val_mae:.4f}\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Val RMSE:   {val_rmse:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Feature importance\n",
        "import pandas as pd\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': minimal_features,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# ============================================================================\n",
        "# 4. PREDICT ON TEST (if needed)\n",
        "# ============================================================================\n",
        "\n",
        "# Uncomment if you want test predictions\n",
        "# X_test = test_df[minimal_features].copy()\n",
        "# test_predictions = model.predict(X_test)\n",
        "# test_df['predicted_target'] = test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled train rows: 999,468\n",
            "\n",
            "Training Improved XGBoost...\n",
            "[0]\tvalidation_0-rmse:880.31271\n",
            "[20]\tvalidation_0-rmse:412.01673\n",
            "[40]\tvalidation_0-rmse:268.37377\n",
            "[60]\tvalidation_0-rmse:220.24941\n",
            "[80]\tvalidation_0-rmse:201.45002\n",
            "[100]\tvalidation_0-rmse:191.31822\n",
            "[120]\tvalidation_0-rmse:182.78789\n",
            "[140]\tvalidation_0-rmse:175.70998\n",
            "[160]\tvalidation_0-rmse:170.49251\n",
            "[180]\tvalidation_0-rmse:165.43575\n",
            "[199]\tvalidation_0-rmse:161.18926\n",
            "\n",
            "======================================================================\n",
            "MODEL PERFORMANCE\n",
            "======================================================================\n",
            "Train MAE:  67.1620\n",
            "Val MAE:    68.1940\n",
            "Train RMSE: 155.1686\n",
            "Val RMSE:   161.1893\n",
            "======================================================================\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "               feature  importance\n",
            "18      target_lag_24h    0.435558\n",
            "19     target_lag_168h    0.127719\n",
            "3       is_consumption    0.119138\n",
            "1          is_business    0.046995\n",
            "5   installed_capacity    0.041308\n",
            "15         day_of_week    0.031271\n",
            "12       temperature_1    0.027415\n",
            "0               county    0.023491\n",
            "10    cloudcover_total    0.020867\n",
            "4            eic_count    0.020273\n",
            "\n",
            "======================================================================\n",
            "MAKING PREDICTIONS ON TEST DATA\n",
            "======================================================================\n",
            "\n",
            "Test predictions made: 528\n",
            "Prediction range: -104.38 to 6632.77\n",
            "Mean prediction: 181.24\n",
            "\n",
            "Sample predictions:\n",
            "                  datetime  county  product_type  is_business  \\\n",
            "18768  2021-10-31 03:00:00       0             0            1   \n",
            "18769  2021-10-31 03:00:00       0             0            1   \n",
            "68160  2022-03-27 03:00:00       0             0            1   \n",
            "68161  2022-03-27 03:00:00       0             0            1   \n",
            "141072 2022-10-30 03:00:00       0             0            1   \n",
            "141073 2022-10-30 03:00:00       0             0            1   \n",
            "190464 2023-03-26 03:00:00       0             0            1   \n",
            "190465 2023-03-26 03:00:00       0             0            1   \n",
            "18762  2021-10-31 03:00:00       0             1            0   \n",
            "18763  2021-10-31 03:00:00       0             1            0   \n",
            "\n",
            "        predicted_target  \n",
            "18768          -3.706432  \n",
            "18769         162.611313  \n",
            "68160           0.548360  \n",
            "68161         293.664368  \n",
            "141072          0.748909  \n",
            "141073        355.013367  \n",
            "190464         11.028194  \n",
            "190465        595.085693  \n",
            "18762          12.545994  \n",
            "18763         118.289658  \n",
            "\n",
            "======================================================================\n",
            "SUBMISSION FILE\n",
            "======================================================================\n",
            "         row_id      target\n",
            "18768    178944   -3.706432\n",
            "18769    178945  162.611313\n",
            "68160    634872    0.548360\n",
            "68161    634873  293.664368\n",
            "141072  1332462    0.748909\n",
            "\n",
            "Total predictions: 528\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# IMPROVED FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "# Time features\n",
        "combined_df['hour'] = pd.to_datetime(combined_df['datetime']).dt.hour\n",
        "combined_df['day_of_week'] = pd.to_datetime(combined_df['datetime']).dt.dayofweek\n",
        "combined_df['month'] = pd.to_datetime(combined_df['datetime']).dt.month\n",
        "combined_df['is_weekend'] = combined_df['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Lag features (historical consumption)\n",
        "combined_df = combined_df.sort_values(['county', 'product_type', 'is_business', 'datetime'])\n",
        "combined_df['target_lag_24h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(24)\n",
        "combined_df['target_lag_168h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(168)\n",
        "\n",
        "# Improved feature set\n",
        "improved_features = [\n",
        "    'county', 'is_business', 'product_type', 'is_consumption',\n",
        "    'eic_count', 'installed_capacity',\n",
        "    'euros_per_mwh', 'lowest_price_per_mwh', 'highest_price_per_mwh',\n",
        "    'temperature', 'cloudcover_total', 'rain',\n",
        "    'temperature_1', 'temperature_2',\n",
        "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
        "    'target_lag_24h', 'target_lag_168h',\n",
        "]\n",
        "\n",
        "# Split\n",
        "train_df = combined_df[combined_df['target'].notna()].copy()\n",
        "test_df = combined_df[combined_df['target'].isna()].copy()\n",
        "\n",
        "# Remove rows with NaN in lag features (first 168 hours)\n",
        "train_df = train_df.dropna(subset=['target_lag_24h', 'target_lag_168h'])\n",
        "\n",
        "# Sample 50% now\n",
        "train_sample = train_df.sample(frac=0.5, random_state=42)\n",
        "print(f\"Sampled train rows: {len(train_sample):,}\")\n",
        "\n",
        "X = train_sample[improved_features].copy()\n",
        "y = train_sample['target'].copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============================================================================\n",
        "# IMPROVED MODEL\n",
        "# ============================================================================\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 3,\n",
        "    'gamma': 0.1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'early_stopping_rounds': 20,\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Improved XGBoost...\")\n",
        "model = xgb.XGBRegressor(**params)\n",
        "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=20)\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "# Metrics\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Train MAE:  {train_mae:.4f}\")\n",
        "print(f\"Val MAE:    {val_mae:.4f}\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Val RMSE:   {val_rmse:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Feature importance\n",
        "import pandas as pd\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': improved_features,  # ← MUUTUS: minimal_features → improved_features\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# ============================================================================\n",
        "# 4. PREDICTIONS ON TEST DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MAKING PREDICTIONS ON TEST DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prepare test features\n",
        "X_test = test_df[improved_features].copy()\n",
        "\n",
        "# Check for NaN in test features\n",
        "test_nan_count = X_test.isnull().sum().sum()\n",
        "if test_nan_count > 0:\n",
        "    print(f\"⚠️ Warning: {test_nan_count} NaN values in test features\")\n",
        "    print(\"Filling NaN with 0...\")\n",
        "    X_test = X_test.fillna(0)\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Add predictions to test dataframe\n",
        "test_df['predicted_target'] = test_predictions\n",
        "\n",
        "print(f\"\\nTest predictions made: {len(test_predictions)}\")\n",
        "print(f\"Prediction range: {test_predictions.min():.2f} to {test_predictions.max():.2f}\")\n",
        "print(f\"Mean prediction: {test_predictions.mean():.2f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample predictions:\")\n",
        "print(test_df[['datetime', 'county', 'product_type', 'is_business', 'predicted_target']].head(10))\n",
        "\n",
        "# ============================================================================\n",
        "# 5. SAVE PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "# Create submission file\n",
        "submission = test_df[['row_id', 'predicted_target']].copy()\n",
        "submission.columns = ['row_id', 'target']\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUBMISSION FILE\")\n",
        "print(\"=\"*70)\n",
        "print(submission.head())\n",
        "print(f\"\\nTotal predictions: {len(submission)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled train rows: 1,398,516\n",
            "\n",
            "Training Improved XGBoost V2...\n",
            "[0]\tvalidation_0-rmse:902.91934\n",
            "[20]\tvalidation_0-rmse:524.58631\n",
            "[40]\tvalidation_0-rmse:326.07719\n",
            "[60]\tvalidation_0-rmse:230.20701\n",
            "[80]\tvalidation_0-rmse:186.34827\n",
            "[100]\tvalidation_0-rmse:165.98828\n",
            "[120]\tvalidation_0-rmse:155.88461\n",
            "[140]\tvalidation_0-rmse:149.58479\n",
            "[160]\tvalidation_0-rmse:145.30097\n",
            "[180]\tvalidation_0-rmse:141.79791\n",
            "[200]\tvalidation_0-rmse:139.60287\n",
            "[220]\tvalidation_0-rmse:137.65919\n",
            "[240]\tvalidation_0-rmse:135.98184\n",
            "[260]\tvalidation_0-rmse:134.36455\n",
            "[280]\tvalidation_0-rmse:132.67342\n",
            "[299]\tvalidation_0-rmse:131.23348\n",
            "\n",
            "======================================================================\n",
            "MODEL PERFORMANCE V2\n",
            "======================================================================\n",
            "Train MAE:  42.1592\n",
            "Val MAE:    43.9662\n",
            "Train RMSE: 118.0630\n",
            "Val RMSE:   131.2335\n",
            "======================================================================\n",
            "\n",
            "Top 15 Most Important Features:\n",
            "                     feature  importance\n",
            "19            target_lag_48h    0.494996\n",
            "18            target_lag_24h    0.207594\n",
            "20            target_lag_72h    0.067318\n",
            "26    capacity_x_consumption    0.056321\n",
            "3             is_consumption    0.044529\n",
            "22   target_rolling_24h_mean    0.016621\n",
            "17                is_weekend    0.010381\n",
            "24               temp_x_hour    0.009878\n",
            "5         installed_capacity    0.009430\n",
            "21           target_lag_168h    0.009354\n",
            "15               day_of_week    0.008958\n",
            "23  target_rolling_168h_mean    0.008908\n",
            "1                is_business    0.007414\n",
            "14                      hour    0.006679\n",
            "13             temperature_2    0.005812\n",
            "\n",
            "======================================================================\n",
            "TEST PREDICTIONS (NON-NEGATIVE)\n",
            "======================================================================\n",
            "Predictions made: 528\n",
            "Prediction range: 0.00 to 6929.65\n",
            "Mean prediction: 177.48\n",
            "Negative predictions fixed: 86\n",
            "\n",
            "Sample predictions:\n",
            "                  datetime  county  product_type  predicted_target\n",
            "18768  2021-10-31 03:00:00       0             0          2.189416\n",
            "18769  2021-10-31 03:00:00       0             0        116.148499\n",
            "68160  2022-03-27 03:00:00       0             0          6.607377\n",
            "68161  2022-03-27 03:00:00       0             0        245.327667\n",
            "141072 2022-10-30 03:00:00       0             0          0.068535\n",
            "141073 2022-10-30 03:00:00       0             0        269.789978\n",
            "190464 2023-03-26 03:00:00       0             0          0.000000\n",
            "190465 2023-03-26 03:00:00       0             0        575.898987\n",
            "18762  2021-10-31 03:00:00       0             1          0.000000\n",
            "18763  2021-10-31 03:00:00       0             1        129.100784\n",
            "\n",
            "✅ Saved to: submission_v2.csv\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ADDITIONAL IMPROVEMENTS\n",
        "# ============================================================================\n",
        "\n",
        "# 1. ADD MORE LAG FEATURES\n",
        "combined_df = combined_df.sort_values(['county', 'product_type', 'is_business', 'datetime'])\n",
        "\n",
        "# Rolling averages (sujuvamad trendid)\n",
        "combined_df['target_rolling_24h_mean'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=24, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "combined_df['target_rolling_168h_mean'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=168, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Lag 48h ja 72h\n",
        "combined_df['target_lag_48h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(48)\n",
        "combined_df['target_lag_72h'] = combined_df.groupby(['county', 'product_type', 'is_business'])['target'].shift(72)\n",
        "\n",
        "# 2. INTERACTION FEATURES\n",
        "combined_df['temp_x_hour'] = combined_df['temperature'] * combined_df['hour']\n",
        "combined_df['price_x_hour'] = combined_df['euros_per_mwh'] * combined_df['hour']\n",
        "combined_df['capacity_x_consumption'] = combined_df['installed_capacity'] * combined_df['is_consumption']\n",
        "\n",
        "# 3. IMPROVED FEATURE SET\n",
        "improved_features_v2 = [\n",
        "    'county', 'is_business', 'product_type', 'is_consumption',\n",
        "    'eic_count', 'installed_capacity',\n",
        "    'euros_per_mwh', 'lowest_price_per_mwh', 'highest_price_per_mwh',\n",
        "    'temperature', 'cloudcover_total', 'rain',\n",
        "    'temperature_1', 'temperature_2',\n",
        "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
        "    'target_lag_24h', 'target_lag_48h', 'target_lag_72h', 'target_lag_168h',\n",
        "    'target_rolling_24h_mean', 'target_rolling_168h_mean',\n",
        "    'temp_x_hour', 'price_x_hour', 'capacity_x_consumption',\n",
        "]\n",
        "\n",
        "# Split\n",
        "train_df = combined_df[combined_df['target'].notna()].copy()\n",
        "test_df = combined_df[combined_df['target'].isna()].copy()\n",
        "\n",
        "# Remove NaN rows\n",
        "train_df = train_df.dropna(subset=improved_features_v2)\n",
        "\n",
        "# Use 70% data now (more is better with more features)\n",
        "train_sample = train_df.sample(frac=0.7, random_state=42)\n",
        "print(f\"Sampled train rows: {len(train_sample):,}\")\n",
        "\n",
        "X = train_sample[improved_features_v2].copy()\n",
        "y = train_sample['target'].copy()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============================================================================\n",
        "# IMPROVED MODEL V2\n",
        "# ============================================================================\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 7,  # 6 → 7\n",
        "    'learning_rate': 0.03,  # 0.05 → 0.03 (aeglasem)\n",
        "    'n_estimators': 300,  # 200 → 300\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 5,  # 3 → 5\n",
        "    'gamma': 0.2,  # 0.1 → 0.2\n",
        "    'reg_alpha': 0.1,  # L1 regularization\n",
        "    'reg_lambda': 1.0,  # L2 regularization\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',\n",
        "    'early_stopping_rounds': 30,  # 20 → 30\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Improved XGBoost V2...\")\n",
        "model = xgb.XGBRegressor(**params)\n",
        "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=20)\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL PERFORMANCE V2\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Train MAE:  {train_mae:.4f}\")\n",
        "print(f\"Val MAE:    {val_mae:.4f}\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Val RMSE:   {val_rmse:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': improved_features_v2,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(feature_importance.head(15))\n",
        "\n",
        "# ============================================================================\n",
        "# PREDICTIONS WITH NON-NEGATIVE CONSTRAINT\n",
        "# ============================================================================\n",
        "\n",
        "X_test = test_df[improved_features_v2].copy()\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# FIX NEGATIVE PREDICTIONS\n",
        "test_predictions = np.maximum(test_predictions, 0)  # ← OLULINE!\n",
        "\n",
        "test_df['predicted_target'] = test_predictions\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST PREDICTIONS (NON-NEGATIVE)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Predictions made: {len(test_predictions)}\")\n",
        "print(f\"Prediction range: {test_predictions.min():.2f} to {test_predictions.max():.2f}\")\n",
        "print(f\"Mean prediction: {test_predictions.mean():.2f}\")\n",
        "print(f\"Negative predictions fixed: {(model.predict(X_test) < 0).sum()}\")\n",
        "\n",
        "# Sample\n",
        "print(\"\\nSample predictions:\")\n",
        "print(test_df[['datetime', 'county', 'product_type', 'predicted_target']].head(10))\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE SUBMISSION\n",
        "# ============================================================================\n",
        "\n",
        "submission = test_df[['row_id', 'predicted_target']].copy()\n",
        "submission.columns = ['row_id', 'target']\n",
        "submission.to_csv('submission_v2.csv', index=False)\n",
        "print(\"\\n✅ Saved to: submission_v2.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

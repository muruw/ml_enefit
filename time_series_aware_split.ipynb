{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# last n days as validation\n",
    "gap_days = 7\n",
    "val_days = 90\n",
    "max_dt = train[\"datetime\"].max()\n",
    "val_start = max_dt - pd.Timedelta(days=val_days)\n",
    "gap_start = val_start - pd.Timedelta(days=gap_days)\n",
    "\n",
    "train_full = train[train[\"datetime\"] < gap_start].copy()\n",
    "valid_full = train[train[\"datetime\"] >= val_start].copy()\n",
    "\n",
    "print(\"Train:\", train_full[\"datetime\"].min(), \"→\", train_full[\"datetime\"].max(), len(train_full))\n",
    "print(\"Valid:\", valid_full[\"datetime\"].min(), \"→\", valid_full[\"datetime\"].max(), len(valid_full))\n",
    "\n",
    "def make_balanced_subset_regression(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"target\",\n",
    "    group_cols=(\"prediction_unit_id\", \"is_consumption\"),\n",
    "    n_bins: int = 20,\n",
    "    frac: float = 0.10,\n",
    "    max_rows: int | None = 300_000,\n",
    "    random_state: int = 343,\n",
    ") -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    parts = []\n",
    "\n",
    "    if max_rows is not None:\n",
    "        frac = min(frac, max_rows / max(len(df), 1))\n",
    "\n",
    "    for _, gdf in df.groupby(list(group_cols), sort=False):\n",
    "        if len(gdf) < 100:\n",
    "            parts.append(gdf)\n",
    "            continue\n",
    "\n",
    "        n_take = int(np.ceil(len(gdf) * frac))\n",
    "        n_take = max(20, min(n_take, len(gdf)))\n",
    "\n",
    "        y = gdf[target_col]\n",
    "        try:\n",
    "            y_binned = pd.qcut(y, q=min(n_bins, len(gdf)), duplicates=\"drop\")\n",
    "            if y_binned.nunique() < 2:\n",
    "                idx = rng.choice(gdf.index.to_numpy(), size=n_take, replace=False)\n",
    "                parts.append(gdf.loc[idx])\n",
    "                continue\n",
    "\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, train_size=n_take, random_state=random_state)\n",
    "            idx_take, _ = next(sss.split(np.zeros(len(gdf)), y_binned))\n",
    "            parts.append(gdf.iloc[idx_take])\n",
    "\n",
    "        except Exception:\n",
    "            idx = rng.choice(gdf.index.to_numpy(), size=n_take, replace=False)\n",
    "            parts.append(gdf.loc[idx])\n",
    "\n",
    "    out = pd.concat(parts, axis=0)\n",
    "\n",
    "    if max_rows is not None and len(out) > max_rows:\n",
    "        out = out.sample(n=max_rows, random_state=random_state)\n",
    "\n",
    "    return out.sort_values([\"prediction_unit_id\",\"is_consumption\",\"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "train_sub = make_balanced_subset_regression(\n",
    "    train_full,\n",
    "    frac=0.10,\n",
    "    max_rows=300_000,\n",
    "    n_bins=20,\n",
    "    random_state=343\n",
    ")\n",
    "\n",
    "print(\"Balanced train subset rows:\", len(train_sub))\n",
    "\n",
    "# X and y for training\n",
    "DROP_COLS = [\n",
    "    \"target\", \"row_id\", \"datetime\"\n",
    "]\n",
    "feature_cols = [c for c in train.columns if c not in DROP_COLS]\n",
    "\n",
    "X_train = train_sub[feature_cols]\n",
    "y_train = train_sub[\"target\"]\n",
    "\n",
    "X_valid = valid_full[feature_cols]\n",
    "y_valid = valid_full[\"target\"]\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_valid:\", X_valid.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
